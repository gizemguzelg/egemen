{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = 0.53\n",
    "\n",
    "sehirler=[\"Adana\", \"Adıyaman\", \"Afyon\", \"Ağrı\", \"Amasya\", \"Ankara\", \"Antalya\", \"Artvin\", \"Aydın\", \"Balıkesir\", \n",
    "          \"Bilecik\", \"Bingöl\", \"Bitlis\", \"Bolu\", \"Burdur\", \"Bursa\", \"Çanakkale\", \"Çankırı\", \"Çorum\", \"Denizli\", \n",
    "          \"Diyarbakır\", \"Edirne\", \"Elazığ\", \"Erzincan\", \"Erzurum\", \"Eskişehir\", \"Gaziantep\", \"Giresun\", \"Gümüşhane\", \n",
    "          \"Hakkari\", \"Hatay\", \"Isparta\", \"Mersin\", \"İstanbul\", \"İzmir\", \"Kars\", \"Kastamonu\", \"Kayseri\", \"Kırklareli\", \n",
    "          \"Kırşehir\", \"Kocaeli\", \"Konya\", \"Kütahya\", \"Malatya\", \"Manisa\", \"Kahramanmaraş\", \"Mardin\", \"Muğla\", \"Muş\", \n",
    "          \"Nevşehir\", \"Niğde\", \"Ordu\", \"Rize\", \"Sakarya\", \"Samsun\", \"Siirt\", \"Sinop\", \"Sivas\", \"Tekirdağ\", \"Tokat\", \n",
    "          \"Trabzon\", \"Tunceli\", \"Şanlıurfa\", \"Uşak\", \"Van\", \"Yozgat\", \"Zonguldak\", \"Aksaray\", \"Bayburt\", \"Karaman\", \n",
    "          \"Kırıkkale\", \"Batman\", \"Şırnak\", \"Bartın\", \"Ardahan\", \"Iğdır\", \"Yalova\", \"Karabük\", \"Kilis\", \"Osmaniye\", \"Düzce\"]\n",
    "\n",
    "for i in range(len(sehirler)):\n",
    "    sehirler[i] = sehirler[i].lower()\n",
    "    sehirler[i] = sehirler[i].replace('i̇','i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy_calculate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calculate(actual_values, predicted_values):\n",
    "\n",
    "  comparison = abs(np.round(predicted_values) - actual_values)\n",
    "  accuracy = 1- ((len(comparison[comparison>=(0+1)])) / len(actual_values))\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean_iou_calculator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_calculator(actual_values, predicted_values, time):\n",
    "  confusion_array = confusion_matrix(actual_values, predicted_values)\n",
    "  individual_ious = []\n",
    "  for i in range(len(confusion_array)):\n",
    "    individual_iou = confusion_array[i][i] / (sum(confusion_array[i]))\n",
    "    individual_ious.append(individual_iou)\n",
    "  mean_iou = sum(individual_ious)/len(individual_ious)\n",
    "  results = pd.DataFrame()  \n",
    "  featue_y_values = sorted(data[time].unique())\n",
    "  for i in range(len(individual_ious)):\n",
    "    results.insert(0, 'iou_(' + str(featue_y_values[i])  +')', [individual_ious[i]], True)\n",
    "  results = results[results.columns[::-1]]\n",
    "  results.insert(0, 'mean_iou', mean_iou, True)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeme\\AppData\\Local\\Temp\\ipykernel_19104\\3280315253.py:30: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data2['town'] = data2['town'].str.replace('.','missing')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>company</th>\n",
       "      <th>amount</th>\n",
       "      <th>town</th>\n",
       "      <th>type</th>\n",
       "      <th>week day</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K477</td>\n",
       "      <td>T-029</td>\n",
       "      <td>1</td>\n",
       "      <td>bursa</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-016</td>\n",
       "      <td>1</td>\n",
       "      <td>istanbul</td>\n",
       "      <td>1</td>\n",
       "      <td>cum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K75</td>\n",
       "      <td>T-018</td>\n",
       "      <td>1</td>\n",
       "      <td>istanbul</td>\n",
       "      <td>1</td>\n",
       "      <td>çrş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-016</td>\n",
       "      <td>1</td>\n",
       "      <td>istanbul</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K513</td>\n",
       "      <td>T-034</td>\n",
       "      <td>1</td>\n",
       "      <td>istanbul</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>K522</td>\n",
       "      <td>T-034</td>\n",
       "      <td>960</td>\n",
       "      <td>istanbul</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>K730</td>\n",
       "      <td>T-060</td>\n",
       "      <td>960</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>K788</td>\n",
       "      <td>T-060</td>\n",
       "      <td>960</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>K1117</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>960</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>K407</td>\n",
       "      <td>T-0133</td>\n",
       "      <td>964</td>\n",
       "      <td>istanbul</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1116 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product company  amount      town  type week day  time\n",
       "0       K477   T-029       1     bursa     1      pts     2\n",
       "1       K669   T-016       1  istanbul     1      cum     4\n",
       "2        K75   T-018       1  istanbul     1      çrş     1\n",
       "3       K669   T-016       1  istanbul     1      pts     1\n",
       "4       K513   T-034       1  istanbul     1      sal     1\n",
       "...      ...     ...     ...       ...   ...      ...   ...\n",
       "1111    K522   T-034     960  istanbul     1      sal     1\n",
       "1112    K730   T-060     960  tekirdağ     1      pts     4\n",
       "1113    K788   T-060     960  tekirdağ     1      pts     4\n",
       "1114   K1117  T-0211     960   kocaeli     1      sal     1\n",
       "1115    K407  T-0133     964  istanbul     1      pts     1\n",
       "\n",
       "[1116 rows x 7 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('gulle.csv', sep=';', encoding = \"utf8\")\n",
    "data.columns = ['product', 'amount', 'company', 'town', 'type', 'order date', 'delivery date', 'time'] #Rearannge the dataframe as the old one\n",
    "\n",
    "data2 = data.drop('delivery date', axis = 1)\n",
    "data2[\"order day\"] = ''\n",
    "data2[\"order month\"] = ''\n",
    "data2[\"week day\"] = ''\n",
    "\n",
    "\n",
    "# Remove 'order date' and add 'order day', 'order month' and 'week day' features\n",
    "for i in range(len(data2)):\n",
    "  data2.at[i, 'order day'] = data2['order date'][i].split()[0]\n",
    "  data2.at[i, 'order month'] = data2['order date'][i].split()[1]\n",
    "  data2.at[i, 'week day'] = data2['order date'][i].split()[-1]\n",
    "data2 = data2.drop('order date', axis = 1)\n",
    "data2['week day'] = data2['week day'].str.replace('Pazartesi','pts')\n",
    "data2['week day'] = data2['week day'].str.replace('Salı','sal')\n",
    "data2['week day'] = data2['week day'].str.replace('Çarşamba','çrş')\n",
    "data2['week day'] = data2['week day'].str.replace('Perşembe','prş')\n",
    "data2['week day'] = data2['week day'].str.replace('Cumartesi','cts')\n",
    "data2['week day'] = data2['week day'].str.replace('Cuma','cum')\n",
    "data2['week day'] = data2['week day'].str.replace('Pazar','paz')\n",
    "\n",
    "# data2 = data2[data2[\"week day\"].str.contains(\"Pazar\") == False]\n",
    "\n",
    "# Rearranging Dataframe\n",
    "data2 = data2[['product', 'company', 'amount', 'town', 'type', 'order day', 'week day', 'order month', 'time']]\n",
    "data2['town'] = data2['town'].str.lower()\n",
    "data2['town'] = data2['town'].str.replace('i̇','i')\n",
    "data2['town'] = data2['town'].str.replace('.','missing')\n",
    "data2['town'] = data2['town'].str.replace(' tekirdağ','tekirdağ')\n",
    "data2['town'] = data2['town'].str.replace('küçükçekmece','istanbul')\n",
    "data2['town'] = data2['town'].str.replace('çorlu','tekirdağ')\n",
    "data2['town'] = data2['town'].str.replace('bandirma','balıkesir')\n",
    "\n",
    "#data2 = data2.drop('town',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.drop('order day',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.drop('order month',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.fillna(\"missing\")\n",
    "#data2 = data2[data2[\"town\"].str.contains(\"missing\") == False]\n",
    "\n",
    "data_clean = data2.copy()\n",
    "drop_df = data2.copy()\n",
    "drop_index_list = []\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>company</th>\n",
       "      <th>amount</th>\n",
       "      <th>town</th>\n",
       "      <th>type</th>\n",
       "      <th>week day</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>60</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>60</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>60</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-0142</td>\n",
       "      <td>60</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>120</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>çrş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>120</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>prş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>120</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>cum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>120</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>120</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>120</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>prş</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>K1046</td>\n",
       "      <td>T-060</td>\n",
       "      <td>360</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    product company  amount      town  type week day  time\n",
       "527   K1046   T-060      60  tekirdağ     1      pts     2\n",
       "534   K1046   T-060      60  tekirdağ     1      pts     1\n",
       "542   K1046   T-060      60  tekirdağ     1      pts     1\n",
       "543   K1046  T-0142      60   missing     1      sal     1\n",
       "704   K1046   T-060     120  tekirdağ     1      çrş     1\n",
       "724   K1046   T-060     120  tekirdağ     1      prş     1\n",
       "728   K1046   T-060     120  tekirdağ     1      cum     3\n",
       "730   K1046   T-060     120  tekirdağ     1      pts     1\n",
       "736   K1046   T-060     120  tekirdağ     1      pts     2\n",
       "746   K1046   T-060     120  tekirdağ     1      prş     4\n",
       "985   K1046   T-060     360  tekirdağ     1      sal     1"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[data2[\"product\"]==\"K1046\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of abroad companies and products that are supplied from abroad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abroad company list:  ['T-0142' 'MZ-401']\n",
      "abroad product list:  ['K730' 'K390' 'MRL313' 'K1046' 'K143' 'K664' 'K9' 'K389' 'K256' 'K395']\n"
     ]
    }
   ],
   "source": [
    "abr = [item for item in data2[\"town\"].unique() if item not in sehirler]\n",
    "abr_str = \"\"\n",
    "\n",
    "for i in range (len(abr)):\n",
    "    abr_str = abr_str + \"|\" + abr[i]\n",
    "abr_str = abr_str[1:]\n",
    "\n",
    "if len(abr) != 0:\n",
    "    data3 = data2[data2[\"town\"].astype('str').str.contains(abr_str) == True]\n",
    "    abr_comp_list = data3[\"company\"].unique()\n",
    "    abr_prod_list = data3[\"product\"].unique()\n",
    "    print(\"abroad company list: \",abr_comp_list)\n",
    "    print(\"abroad product list: \",abr_prod_list)\n",
    "else:\n",
    "    print(\"no abroad company\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove insufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_insuff(df, ft):\n",
    "    print(\"removing insufficient data for \", ft, \"...\")\n",
    "    fst_len = len(df)\n",
    "    x = df[ft].value_counts() < 5 \n",
    "    df2 = x.to_frame().reset_index()\n",
    "    df2.columns = [ft, 'booly']\n",
    "    df2.drop(df2[df2.booly == False].index, inplace=True)\n",
    "    drop_list = df2[ft].tolist()\n",
    "    drop_indices=[]\n",
    "\n",
    "    if len(drop_list) != 0:\n",
    "        for i in df.index:\n",
    "            for j in range(len(drop_list)):\n",
    "                if (drop_list[j] == df.at[i, ft]):\n",
    "                    drop_indices = drop_indices + [i]\n",
    "        df.drop(drop_indices, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        drop_indices = []\n",
    "                        \n",
    "    lst_len = len(df)\n",
    "    rem = fst_len - lst_len      # number of removed data\n",
    "    per = (rem / fst_len) * 100  # percentage of removed data\n",
    "\n",
    "    print(\"total number of removed data: \", rem)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    return df, drop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing insufficient data for  company ...\n",
      "total number of removed data:  32\n",
      "persentage of removed data:  2.87 %\n",
      "removing insufficient data for  product ...\n",
      "total number of removed data:  169\n",
      "persentage of removed data:  15.59 %\n"
     ]
    }
   ],
   "source": [
    "data2, drop_indices = remove_insuff(data2, \"company\")\n",
    "drop_index_list = drop_index_list + drop_indices\n",
    "\n",
    "data2, drop_indices = remove_insuff(data2, \"product\")\n",
    "drop_index_list = drop_index_list + drop_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_noise(df): \n",
    "    in_len = len(df)\n",
    "    zs = 0.89\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "    \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                max_min_a = df_max_scaled2[\"amount\"].max() - df_max_scaled2[\"amount\"].min()\n",
    "                \n",
    "                if (max_min_a != 0) and (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zs\n",
    "\n",
    "                    df_max_scaled2[\"amount\"] = (df_max_scaled2[\"amount\"] - df_max_scaled2[\"amount\"].min()) / max_min_a\n",
    "                    amo_sc = df_max_scaled2[\"amount\"]\n",
    "                    df_zscore_a = (amo_sc - amo_sc.mean())/amo_sc.std()\n",
    "                    dfz_a = abs(df_zscore_a) > zs\n",
    "\n",
    "                    df1 = dfz_t[\"time\"] & dfz_a \n",
    "                    df2 = (df_zscore_t[\"time\"] * df_zscore_a) < 0 \n",
    "                    dfz = df1 & df2 \n",
    "\n",
    "                    index_drop_list = index_drop_list + [*filter(dfz.get, dfz.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_noise_tt(df): \n",
    "    in_len = len(df)\n",
    "    zst = 1.2\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "        \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                \n",
    "                if (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zst\n",
    "                    \n",
    "                    index_drop_list = index_drop_list + dfz_t[dfz_t[\"time\"].eq(True)].index.tolist()\n",
    "                    \n",
    "                    #index_drop_list = index_drop_list + [*filter(dfz_t.get, dfz_t.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning noise ... \n",
      "deleted indices:  [228, 254, 255, 358, 372, 492, 494, 508, 532, 547, 549, 564, 663, 684, 701, 737, 744, 787, 854, 956, 1024, 1037, 1088, 1089]\n",
      "total number of removed data:  24\n",
      "persentage of removed data:  2.62 %\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "data2, train_drop_list = clean_noise(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning noise ... \n",
      "deleted indices:  [23, 28, 34, 53, 91, 94, 103, 111, 114, 118, 128, 135, 139, 185, 194, 195, 196, 197, 209, 218, 236, 238, 247, 284, 286, 287, 292, 306, 307, 308, 312, 316, 333, 336, 347, 362, 388, 394, 454, 458, 462, 470, 474, 502, 540, 541, 552, 566, 628, 637, 655, 659, 675, 720, 726, 728, 745, 746, 757, 763, 788, 798, 807, 812, 831, 839, 840, 851, 856, 871, 879, 882, 899, 914, 917, 919, 920, 947, 950, 975, 977, 981, 1005, 1019, 1028, 1035, 1065, 1066, 1082]\n",
      "total number of removed data:  89\n",
      "persentage of removed data:  9.99 %\n"
     ]
    }
   ],
   "source": [
    "data2, train_drop_list = clean_noise_tt(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>company</th>\n",
       "      <th>amount</th>\n",
       "      <th>town</th>\n",
       "      <th>type</th>\n",
       "      <th>week day</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K477</td>\n",
       "      <td>T-029</td>\n",
       "      <td>1</td>\n",
       "      <td>bursa</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>1</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>cum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>1</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>1</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>1</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>prş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product company  amount     town  type week day  time\n",
       "0    K477   T-029       1    bursa     1      pts     2\n",
       "5    K669  T-0211       1  kocaeli     1      cum     3\n",
       "6    K669  T-0211       1  kocaeli     1      pts     3\n",
       "7    K669  T-0211       1  kocaeli     1      pts     1\n",
       "8    K669  T-0211       1  kocaeli     1      prş     1"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9444873403305585"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[\"time\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1134663341645887"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[\"time\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.14000000000001"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)*0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.21052631578947"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2) / len(data2.groupby('time').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "1     496\n",
       "2     119\n",
       "3      84\n",
       "4      45\n",
       "5      21\n",
       "6      12\n",
       "7       5\n",
       "8       2\n",
       "9       4\n",
       "10      3\n",
       "11      2\n",
       "12      1\n",
       "14      1\n",
       "18      1\n",
       "19      2\n",
       "26      1\n",
       "27      1\n",
       "34      1\n",
       "44      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.groupby('time').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_time(df):\n",
    "    \n",
    "    min_samp = 132 #len(df)*0.07\n",
    "    t = 1\n",
    "    gs = 0\n",
    "    group = []\n",
    "    sub_group = [t]\n",
    "    gap = 5\n",
    "\n",
    "    while t <= df[\"time\"].max():\n",
    "        \n",
    "        \n",
    "        if sum(df[\"time\"] == t) > 0:\n",
    "        \n",
    "            gs += sum(df[\"time\"] == t)\n",
    "\n",
    "            if (len(sub_group) > 0) and ((t - min(sub_group)) <= gap):\n",
    "                sub_group += [t]\n",
    "\n",
    "            else:\n",
    "                if (len(sub_group) != 0):\n",
    "                    group += [sub_group]\n",
    "                sub_group = [t]\n",
    "\n",
    "            if (gs >= min_samp) and ((t - min(sub_group)) <= gap):\n",
    "                gs = 0\n",
    "                group += [sub_group]\n",
    "                sub_group = []\n",
    "                \n",
    "        if t == df[\"time\"].max() :\n",
    "            group += [sub_group]\n",
    "            group[0].remove(1)\n",
    "            \n",
    "            c = True    # son sub_grouptan bir önceki sub_groupa time aktarımı\n",
    "            gap = 5     # son sub_grouptan bir önceki sub_groupa yollanan time değerleri arasındaki maksimum fark \n",
    "            while c:\n",
    "                if len(group[- 1]) >= 2:\n",
    "                    x = group[-1][0]\n",
    "                    x_l = group[-2] [-1]\n",
    "                    x_r = group[-1] [1]\n",
    "\n",
    "                    if (x - group[-2][0]) <= gap:\n",
    "                        group[-2].append(x)\n",
    "                        group[-1].remove(x)\n",
    "                \n",
    "                x = group[-1][0]\n",
    "                if (x - group[-2][0]) <= gap:\n",
    "                    group[-2].append(x)\n",
    "                    group[-1].remove(x)\n",
    "                    \n",
    "                else:\n",
    "                    c = False\n",
    "            \n",
    "            c = True     # son sub_groupta bulunan zamanlar dataframede 5 kereden az bulunuyorsa bu sub_groupu sil\n",
    "            while c:\n",
    "                s = 0\n",
    "                for i in range(len(group[-1])):\n",
    "                    s += sum(df[\"time\"] == group[-1][i])\n",
    "                if s < 5:\n",
    "                    group.pop(-1)\n",
    "                else:\n",
    "                    c = False\n",
    "                \n",
    "                \n",
    "        \n",
    "        t += 1\n",
    "        \n",
    "    return group\n",
    "\n",
    "\n",
    "############# son sub_groupta kalan datalara bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = group_time(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2, 3], [4, 5, 6, 7, 8, 9], [10, 11, 12, 14]]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time(df, groups):\n",
    "    time = 0\n",
    "    for sub_group in groups:\n",
    "        time += 1\n",
    "        for t in sub_group:\n",
    "            for i in df.index:\n",
    "                if df.at[i, \"time\"] == t:\n",
    "                    df.at[i, \"time\"] = time\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>company</th>\n",
       "      <th>amount</th>\n",
       "      <th>town</th>\n",
       "      <th>type</th>\n",
       "      <th>week day</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K477</td>\n",
       "      <td>T-029</td>\n",
       "      <td>1</td>\n",
       "      <td>bursa</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>1</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>cum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>1</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>1</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>K669</td>\n",
       "      <td>T-0211</td>\n",
       "      <td>1</td>\n",
       "      <td>kocaeli</td>\n",
       "      <td>1</td>\n",
       "      <td>prş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>K730</td>\n",
       "      <td>T-0142</td>\n",
       "      <td>960</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>prş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>K522</td>\n",
       "      <td>T-034</td>\n",
       "      <td>960</td>\n",
       "      <td>istanbul</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>K730</td>\n",
       "      <td>T-060</td>\n",
       "      <td>960</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>K788</td>\n",
       "      <td>T-060</td>\n",
       "      <td>960</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>K407</td>\n",
       "      <td>T-0133</td>\n",
       "      <td>964</td>\n",
       "      <td>istanbul</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>802 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     product company  amount      town  type week day  time\n",
       "0       K477   T-029       1     bursa     1      pts     2\n",
       "5       K669  T-0211       1   kocaeli     1      cum     2\n",
       "6       K669  T-0211       1   kocaeli     1      pts     2\n",
       "7       K669  T-0211       1   kocaeli     1      pts     1\n",
       "8       K669  T-0211       1   kocaeli     1      prş     1\n",
       "...      ...     ...     ...       ...   ...      ...   ...\n",
       "1110    K730  T-0142     960   missing     1      prş     1\n",
       "1111    K522   T-034     960  istanbul     1      sal     1\n",
       "1112    K730   T-060     960  tekirdağ     1      pts     3\n",
       "1113    K788   T-060     960  tekirdağ     1      pts     3\n",
       "1115    K407  T-0133     964  istanbul     1      pts     1\n",
       "\n",
       "[802 rows x 7 columns]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_time(data2, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  1,  3,  4, 26, 34, 44, 19, 27, 18], dtype=int64)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[\"time\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msr\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sr' is not defined"
     ]
    }
   ],
   "source": [
    "stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time(df):\n",
    "    \n",
    "    min_samp = 5\n",
    "    t = 0\n",
    "    gs = 0\n",
    "    group = []\n",
    "    sub_group = []\n",
    "    \n",
    "    while t <= df[\"time\"].max():\n",
    "        \n",
    "        t += 1\n",
    "        if sum(df[\"time\"]==t) > 0\n",
    "        \n",
    "            sub_group = [t]\n",
    "            gs += sum(df[\"time\"]==t)\n",
    "\n",
    "            if (len(sub_group) > 0) and ((max(sub_group) - min(sub_group)) < 7):\n",
    "                sub_group += [t]\n",
    "                print(\"a\")\n",
    "            else:\n",
    "                if len(sub_group) != 0:\n",
    "                    group += [sub_group]\n",
    "                sub_group = [t]\n",
    "                print(\"b\")\n",
    "\n",
    "            if (gs >= min_samp):\n",
    "                print(\"c\")\n",
    "                gs = 0\n",
    "                group += [sub_group]\n",
    "                sub_group = []\n",
    "            \n",
    "        \n",
    "        \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time(df):\n",
    "    for i in df.index:\n",
    "        if ((df.at[i, \"time\"]<=7) and (df.at[i, \"time\"]>=4)):\n",
    "            df.at[i, \"time\"] = 4\n",
    "        elif (df.at[i, \"time\"]>7):\n",
    "            df.at[i, \"time\"] = 5\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time(df):\n",
    "    for i in df.index:\n",
    "        if ((df.at[i, \"time\"]<=4) and (df.at[i, \"time\"]>=3)):\n",
    "            df.at[i, \"time\"] = 3\n",
    "        elif ((df.at[i, \"time\"]<=7) and (df.at[i, \"time\"]>=5)):\n",
    "            df.at[i, \"time\"] = 4\n",
    "        elif ((df.at[i, \"time\"]<=14) and (df.at[i, \"time\"]>=8)):\n",
    "            df.at[i, \"time\"] = 5\n",
    "        elif ((df.at[i, \"time\"]<=30) and (df.at[i, \"time\"]>=15)):\n",
    "            df.at[i, \"time\"] = 6\n",
    "        elif (df.at[i, \"time\"]>30):\n",
    "            df.at[i, \"time\"] = 7\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = map_time(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One - Hot - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(df, ft):      ### ft = \"company\", \"product\", \"week day\" etc.\n",
    "    print(\"one hot encoding \", ft, \"...\")\n",
    "    dum = pd.get_dummies(df[ft])\n",
    "    df = df.drop(ft, axis = 1)\n",
    "    df = df.join(dum)\n",
    "    print(ft, \"encoded.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data2  = one_hot(data2, \"week day\")\n",
    "data2  = one_hot(data2, \"company\")\n",
    "data2  = one_hot(data2, \"product\")\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2.drop(train_drop_list, axis=0, inplace=True)\n",
    "#data2.drop(val_drop_list, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index_list = sorted(list(set(drop_index_list)))\n",
    "drop_df = drop_df.loc[drop_index_list]\n",
    "drop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.drop(drop_df.index.to_list(), axis=0, inplace=True)\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop town column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.drop('town',axis = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import EditedNearestNeighbours \n",
    "from collections import Counter\n",
    "\n",
    "xt = data2.copy()\n",
    "yt = data2.copy()\n",
    "xt.drop(\"time\", axis=1, inplace=True)\n",
    "yt = yt[\"time\"] \n",
    "\n",
    "\n",
    "print('Original dataset shape %s' % Counter(xt))\n",
    "print(f'Original dataset samples per class {Counter(yt)}')\n",
    "\n",
    "sm_obj = SMOTE(random_state=42, sampling_strategy='all', k_neighbors=4, n_jobs=-1)\n",
    "enn_obj = EditedNearestNeighbours(sampling_strategy='all', n_neighbors=3, kind_sel='mode', n_jobs=-1)\n",
    "sme = SMOTEENN(random_state=42, smote=sm_obj, enn=enn_obj, sampling_strategy='all', n_jobs=-1)\n",
    "\n",
    "X_res, y_res = sme.fit_resample(xt, yt)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))\n",
    "print(f'Resampled dataset shape {X_res.shape}')\n",
    "x_train, y_train = X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data2.copy()\n",
    "Y = data2.copy()\n",
    "X.drop(\"time\", axis=1, inplace=True)\n",
    "Y = Y[\"time\"]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,Y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(df):\n",
    "    ls = df.columns.to_list()\n",
    "    ls.remove(\"amount\")\n",
    "    df[ls] = df[ls].astype('category')\n",
    "#     df['time'] = df['time'].cat.rename_categories({1:\"1 gün\", 2:\"2 gün\", 3:\"3-4 gün\", 4:\"5-7 gün\", 5:\"8-14 gün\", 6:\"15-30 gün\", 7:\"+30 gün\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = categorize(x_train)\n",
    "# x_val = categorize(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_min = x_train[\"amount\"].min()\n",
    "xt_max = x_train[\"amount\"].max()\n",
    "\n",
    "x_train[\"amount\"] = (x_train[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "x_val[\"amount\"] = (x_val[\"amount\"] - xt_min) / (xt_max - xt_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a df_empty with the same columns of x_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empty = x_train[0:0]\n",
    "df_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(x_train, x_val):\n",
    "    from sklearn.decomposition import PCA\n",
    "    xl = len(x_train.columns)\n",
    "    pca = PCA(.95)\n",
    "    pca.fit(x_train)\n",
    "    print(\"number of features dropped from \", xl, \" to \", pca.n_components_) \n",
    "    #print(\"variance ratio: \", pca.explained_variance_ratio_) \n",
    "\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_val = pca.transform(x_val)\n",
    "    return pca, x_train, x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTENC\n",
    "##### before pca for sure (there are categorical fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_fts(df):\n",
    "    fts = list(df.columns)\n",
    "    fts.remove('amount')\n",
    "    return fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTENC\n",
    "# from collections import Counter\n",
    "\n",
    "# print(f'Original dataset shape {x_train.shape}')\n",
    "# print(f'Original dataset samples per class {Counter(y_train)}')\n",
    "\n",
    "# # categorical features\n",
    "# fts = list(range(1, ((x_train.shape[1]))))\n",
    "\n",
    "# sm = SMOTENC(random_state=42, categorical_features=fts, sampling_strategy='all', k_neighbors=4, n_jobs=-1)\n",
    "# X_res, y_res = sm.fit_resample(x_train, y_train)\n",
    "# print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "# print(f'Resampled dataset shape {X_res.shape}')\n",
    "# x_train, y_train = X_res, y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTEENN\n",
    "##### after pca?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, x_train, x_val = do_pca(x_train, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTEENN\n",
    "# from imblearn.over_sampling import SMOTE \n",
    "# from imblearn.under_sampling import EditedNearestNeighbours \n",
    "# from collections import Counter\n",
    "# print('Original dataset shape %s' % Counter(y_train))\n",
    "# print(f'Original dataset samples per class {Counter(y_train)}')\n",
    "\n",
    "# sm_obj = SMOTE(random_state=42, sampling_strategy='all', k_neighbors=4, n_jobs=-1)\n",
    "# enn_obj = EditedNearestNeighbours(sampling_strategy='all', n_neighbors=3, kind_sel='mode', n_jobs=-1)\n",
    "# sme = SMOTEENN(random_state=42, smote=sm_obj, enn=enn_obj, sampling_strategy='all', n_jobs=-1)\n",
    "# X_res, y_res = sme.fit_resample(x_train, y_train)\n",
    "# print('Resampled dataset shape %s' % Counter(y_res))\n",
    "# print(f'Resampled dataset shape {X_res.shape}')\n",
    "# x_train, y_train = X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 7.778449929951673,\n",
    " 'min_samples_leaf': 3.343093555340267,\n",
    " 'min_samples_split': 4.450238161602154,\n",
    " 'n_estimators': 267.8280516713154}\n",
    "\n",
    "params[\"min_samples_leaf\"] = round(params[\"min_samples_leaf\"])\n",
    "params[\"min_samples_split\"] = round(params[\"min_samples_split\"])\n",
    "params[\"n_estimators\"] = round(params[\"n_estimators\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndmForest = RandomForestClassifier(random_state=42).fit(x_train, y_train)\n",
    "rndmForest_opt = RandomForestClassifier(random_state=42, **params).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndmForest_predictions = rndmForest.predict(x_val)\n",
    "rndmForest_opt_predictions = rndmForest_opt.predict(x_val)\n",
    "\n",
    "print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_val, rndmForest_predictions))\n",
    "print('Accuracy of RandomForest_opt classifier: ', accuracy_calculate(y_val, rndmForest_opt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, rndmForest_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, rndmForest_opt_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "\n",
    "Logistic_Regression = LogisticRegression(random_state= random_state, multi_class='multinomial', max_iter = 1000).fit(x_train, y_train.astype('int'))\n",
    "#linearReg = LinearRegression().fit(x_train, y_train.astype('int'))\n",
    "rndmForest = RandomForestClassifier(n_estimators=1000, class_weight=\"balanced\", n_jobs=-1, min_samples_leaf=3, max_depth=5,\n",
    "                                    random_state=random_state).fit(x_train, y_train.astype('int'))\n",
    "                                                                   \n",
    "#MLP = MLPClassifier(random_state=1, max_iter=5000).fit(x_train, y_train.astype('int'))\n",
    "\n",
    "#Finding k value fom max accuracy\n",
    "k_values=[]\n",
    "for k in range(1, 51):\n",
    "    KNeighbors = KNeighborsClassifier(n_neighbors=k, weights='distance').fit(x_train, y_train.astype('int'))\n",
    "    KNeighbors_predictions = KNeighbors.predict(x_val)\n",
    "    k_values.append(balanced_accuracy_score(y_val, KNeighbors_predictions))\n",
    "k_max = k_values.index(max(k_values)) + 1\n",
    "\n",
    "KNeighbors = KNeighborsClassifier(n_neighbors=k_max).fit(x_train, y_train.astype('int'))\n",
    "CSupportVector = make_pipeline(StandardScaler(), SVC(gamma='auto')).fit(x_train, y_train.astype('int'))\n",
    "DecisionTtree = DecisionTreeClassifier(random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# GaussianProcess = GaussianProcessClassifier(kernel=kernel,random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# AdaBoost = AdaBoostClassifier(n_estimators=1000, random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# GaussianNaiveBayes = GaussianNB().fit(x_train, y_train.astype('int'))\n",
    "# QuadraticDiscriminantAnalysis = QuadraticDiscriminantAnalysis().fit(x_train, y_train.astype('int'))\n",
    "\n",
    "\n",
    "LogisticRegression_predictions = Logistic_Regression.predict(x_train)\n",
    "#linearReg_predictions = linearReg.predict(x_val)\n",
    "rndmForest_predictions = rndmForest.predict(x_train)\n",
    "#MLP_predictions = MLP.predict(x_val)\n",
    "KNeighbors_predictions = KNeighbors.predict(x_train)\n",
    "CSupportVector_predictions = CSupportVector.predict(x_train)\n",
    "DecisionTtree_predictions = DecisionTtree.predict(x_train)\n",
    "# GaussianProcess_predictions = GaussianProcess.predict(x_val)\n",
    "# AdaBoost_predictions = AdaBoost.predict(x_val)\n",
    "# GaussianNaiveBayes_predictions = GaussianNaiveBayes.predict(x_val)\n",
    "# QuadraticDiscriminantAnalysis_predictions = QuadraticDiscriminantAnalysis.predict(x_val)\n",
    "\n",
    "\n",
    "print('Accuracy of LogisticRegression classifier: ', accuracy_calculate(y_train, LogisticRegression_predictions))\n",
    "#print('Accuracy of LinearRegression classifier: ', accuracy_calculate(y_val, linearReg_predictions))\n",
    "print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_train, rndmForest_predictions))\n",
    "#print('Accuracy of Multi-layer Perceptron classifier: ', accuracy_calculate(y_val, MLP_predictions))\n",
    "print('Accuracy of KNeighbors classifier: ', accuracy_calculate(y_train, KNeighbors_predictions))\n",
    "print('Accuracy of CSupportVector classifier: ', accuracy_calculate(y_train, CSupportVector_predictions))\n",
    "print('Accuracy of DecisionTtree classifier: ', accuracy_calculate(y_train, DecisionTtree_predictions))\n",
    "#print('Accuracy of GaussianProcess classifier: ', accuracy_calculate(y_val, GaussianProcess_predictions))\n",
    "#print('Accuracy of AdaBoost classifier: ', accuracy_calculate(y_val, AdaBoost_predictions))\n",
    "#print('Accuracy of GaussianNaiveBayes classifier: ', accuracy_calculate(y_val, GaussianNaiveBayes_predictions))\n",
    "#print('Accuracy of QuadraticDiscriminantAnalysis classifier: ', accuracy_calculate(y_val, QuadraticDiscriminantAnalysis_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Accuracy / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Balanced_Accuracy of LogisticRegression classifier: ', balanced_accuracy_score(y_train, LogisticRegression_predictions))\n",
    "print('Balanced_Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_train, rndmForest_predictions))\n",
    "print('Balanced_Accuracy of KNeighbors classifier: ', balanced_accuracy_score(y_train, KNeighbors_predictions))\n",
    "print('Balanced_Accuracy of CSupportVector classifier: ', balanced_accuracy_score(y_train, CSupportVector_predictions))\n",
    "print('Balanced_Accuracy of DecisionTtree classifier: ', balanced_accuracy_score(y_train, DecisionTtree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DENİYOZ İŞTE #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTtree = DecisionTreeClassifier(random_state=0, criterion='gini', min_samples_leaf=3, max_depth=20).fit(x_train, y_train.astype('int'))\n",
    "# DecisionTtree_predictions = DecisionTtree.predict(x_train)\n",
    "# print('Accuracy of DecisionTtree_predictions : ', accuracy_calculate(y_train, DecisionTtree_predictions))\n",
    "# print('Balanced_Accuracy of DecisionTtree_predictions : ', balanced_accuracy_score(y_train, DecisionTtree_predictions))\n",
    "\n",
    "# DecisionTtree_predictions = DecisionTtree.predict(x_val)\n",
    "# print('Accuracy of DecisionTtree_predictions : ', accuracy_calculate(y_val, DecisionTtree_predictions))\n",
    "# print('Balanced_Accuracy of DecisionTtree_predictions : ', balanced_accuracy_score(y_val, DecisionTtree_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic_Regression = LogisticRegression(class_weight=\"balanced\", multi_class=\"ovr\", random_state= random_state, max_iter = 1000).fit(x_train, y_train.astype('int'))\n",
    "# LogisticRegression_predictions = Logistic_Regression.predict(x_train)\n",
    "# print('Accuracy of LogisticRegression classifier: ', accuracy_calculate(y_train, LogisticRegression_predictions))\n",
    "# print('Balanced_Accuracy of LogisticRegression classifier: ', balanced_accuracy_score(y_train, LogisticRegression_predictions))\n",
    "\n",
    "# LogisticRegression_predictions = Logistic_Regression.predict(x_val)\n",
    "# print('Accuracy of LogisticRegression classifier: ', accuracy_calculate(y_val, LogisticRegression_predictions))\n",
    "# print('Balanced_Accuracy of LogisticRegression classifier: ', balanced_accuracy_score(y_val, LogisticRegression_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SGDClassifier(loss='log',class_weight=\"balanced\", max_iter=2000).fit(x_train, y_train)\n",
    "# predictions = svm.predict(x_val)\n",
    "# print(\"accuracy: \",accuracy_score(predictions, y_val))\n",
    "# print(\"accuracy: \",balanced_accuracy_score(predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_train, LogisticRegression_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_train, LogisticRegression_predictions))\n",
    "print(\"precision: \",precision_score(y_train, LogisticRegression_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_train, LogisticRegression_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_train, LogisticRegression_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_train, LogisticRegression_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_train, LogisticRegression_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_train, rndmForest_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_train, rndmForest_predictions))\n",
    "print(\"precision: \",precision_score(y_train, rndmForest_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_train, rndmForest_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_train, rndmForest_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_train, rndmForest_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_train, rndmForest_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier  / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_train, KNeighbors_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_train, KNeighbors_predictions))\n",
    "print(\"precision: \",precision_score(y_train, KNeighbors_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_train, KNeighbors_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_train, KNeighbors_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_train, KNeighbors_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_train, rndmForest_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSupportVector  / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_train, CSupportVector_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_train, CSupportVector_predictions))\n",
    "print(\"precision: \",precision_score(y_train, CSupportVector_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_train, CSupportVector_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_train, CSupportVector_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_train, CSupportVector_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_train, CSupportVector_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random_state = 42\n",
    "# # kernel = 1.0 * RBF(1.0)\n",
    "\n",
    "# # Logistic_Regression = LogisticRegression(random_state= random_state, max_iter = 1000).fit(x_train, y_train.astype('int'))\n",
    "# # #linearReg = LinearRegression().fit(x_train, y_train.astype('int'))\n",
    "# # rndmForest = RandomForestClassifier(n_estimators=1000, class_weight=\"balanced\", n_jobs=-1, min_samples_leaf=3, max_depth=5,\n",
    "# #                                     random_state=random_state).fit(x_train, y_train.astype('int'))\n",
    "                                                                   \n",
    "# # #MLP = MLPClassifier(random_state=1, max_iter=5000).fit(x_train, y_train.astype('int'))\n",
    "\n",
    "# # #Finding k value fom max accuracy\n",
    "# # k_values=[]\n",
    "# # for k in range(1, 51):\n",
    "# #     KNeighbors = KNeighborsClassifier(n_neighbors=k).fit(x_train, y_train.astype('int'))\n",
    "# #     KNeighbors_predictions = KNeighbors.predict(x_val)\n",
    "# #     k_values.append(accuracy_calculate(y_val, KNeighbors_predictions))\n",
    "\n",
    "# # k_max = k_values.index(max(k_values)) + 1\n",
    "# KNeighbors = KNeighborsClassifier(n_neighbors=k_max).fit(x_train, y_train.astype('int'))\n",
    "# CSupportVector = make_pipeline(StandardScaler(), SVC(gamma='auto')).fit(x_train, y_train.astype('int'))\n",
    "# DecisionTtree = DecisionTreeClassifier(random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# # GaussianProcess = GaussianProcessClassifier(kernel=kernel,random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# # AdaBoost = AdaBoostClassifier(n_estimators=1000, random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# # GaussianNaiveBayes = GaussianNB().fit(x_train, y_train.astype('int'))\n",
    "# # QuadraticDiscriminantAnalysis = QuadraticDiscriminantAnalysis().fit(x_train, y_train.astype('int'))\n",
    "\n",
    "\n",
    "LogisticRegression_predictions = Logistic_Regression.predict(x_val)\n",
    "#linearReg_predictions = linearReg.predict(x_val)\n",
    "rndmForest_predictions = rndmForest.predict(x_val)\n",
    "#MLP_predictions = MLP.predict(x_val)\n",
    "KNeighbors_predictions = KNeighbors.predict(x_val)\n",
    "CSupportVector_predictions = CSupportVector.predict(x_val)\n",
    "DecisionTtree_predictions = DecisionTtree.predict(x_val)\n",
    "# GaussianProcess_predictions = GaussianProcess.predict(x_val)\n",
    "# AdaBoost_predictions = AdaBoost.predict(x_val)\n",
    "# GaussianNaiveBayes_predictions = GaussianNaiveBayes.predict(x_val)\n",
    "# QuadraticDiscriminantAnalysis_predictions = QuadraticDiscriminantAnalysis.predict(x_val)\n",
    "\n",
    "\n",
    "print('Accuracy of LogisticRegression classifier: ', accuracy_calculate(y_val, LogisticRegression_predictions))\n",
    "#print('Accuracy of LinearRegression classifier: ', accuracy_calculate(y_val, linearReg_predictions))\n",
    "print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_val, rndmForest_predictions))\n",
    "#print('Accuracy of Multi-layer Perceptron classifier: ', accuracy_calculate(y_val, MLP_predictions))\n",
    "print('Accuracy of KNeighbors classifier: ', accuracy_calculate(y_val, KNeighbors_predictions))\n",
    "print('Accuracy of CSupportVector classifier: ', accuracy_calculate(y_val, CSupportVector_predictions))\n",
    "print('Accuracy of DecisionTtree classifier: ', accuracy_calculate(y_val, DecisionTtree_predictions))\n",
    "#print('Accuracy of GaussianProcess classifier: ', accuracy_calculate(y_val, GaussianProcess_predictions))\n",
    "#print('Accuracy of AdaBoost classifier: ', accuracy_calculate(y_val, AdaBoost_predictions))\n",
    "#print('Accuracy of GaussianNaiveBayes classifier: ', accuracy_calculate(y_val, GaussianNaiveBayes_predictions))\n",
    "#print('Accuracy of QuadraticDiscriminantAnalysis classifier: ', accuracy_calculate(y_val, QuadraticDiscriminantAnalysis_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rndmForest = RandomForestClassifier(n_estimators=1000, class_weight=\"balanced\", n_jobs=-1, min_samples_leaf=3, max_depth=5,\n",
    "#                                     random_state=random_state).fit(x_train, y_train.astype('int'))\n",
    "# rndmForest_predictions = rndmForest.predict(x_val)\n",
    "# print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_val, rndmForest_predictions))\n",
    "# print('Balanced_Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_val, rndmForest_predictions))\n",
    "# rndmForest_predictions = rndmForest.predict(x_train)\n",
    "# print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_train, rndmForest_predictions))\n",
    "# print('Balanced_Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_train, rndmForest_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Accuracy / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Balanced_Accuracy of LogisticRegression classifier: ', balanced_accuracy_score(y_val, LogisticRegression_predictions))\n",
    "print('Balanced_Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_val, rndmForest_predictions))\n",
    "print('Balanced_Accuracy of KNeighbors classifier: ', balanced_accuracy_score(y_val, KNeighbors_predictions))\n",
    "print('Balanced_Accuracy of CSupportVector classifier: ', balanced_accuracy_score(y_val, CSupportVector_predictions))\n",
    "print('Balanced_Accuracy of DecisionTtree classifier: ', balanced_accuracy_score(y_val, DecisionTtree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_val, LogisticRegression_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_val, LogisticRegression_predictions))\n",
    "print(\"precision: \",precision_score(y_val, LogisticRegression_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_val, LogisticRegression_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_val, LogisticRegression_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_val, LogisticRegression_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_val, LogisticRegression_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_val, rndmForest_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_val, rndmForest_predictions))\n",
    "print(\"precision: \",precision_score(y_val, rndmForest_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_val, rndmForest_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_val, rndmForest_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_val, rndmForest_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_val, rndmForest_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_val, KNeighbors_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_val, KNeighbors_predictions))\n",
    "print(\"precision: \",precision_score(y_val, KNeighbors_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_val, KNeighbors_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_val, KNeighbors_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_val, KNeighbors_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_val, KNeighbors_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSupportVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_val, CSupportVector_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_val, CSupportVector_predictions))\n",
    "print(\"precision: \",precision_score(y_val, CSupportVector_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_val, CSupportVector_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_val, CSupportVector_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_val, CSupportVector_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_val, CSupportVector_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take input and create df_inp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prod_in_data(df):\n",
    "    prods = df[\"product\"].unique()\n",
    "    prd = str(input(\"Product seç: \"))\n",
    "    if prd not in prods: \n",
    "        return False, prd\n",
    "    else:\n",
    "        return True, prd\n",
    "    \n",
    "def is_prod_in_data_drop(df, prd):\n",
    "    prods = df[\"product\"].unique()\n",
    "    if prd not in prods: \n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_input(df, prd):\n",
    "    amo = input(\"Amount: \") \n",
    "    y = False\n",
    "    while y == False:\n",
    "        if (amo.isnumeric() == False):\n",
    "            print(\"Pozitif tam sayı değer giriniz\")\n",
    "            amo = input(\"Amount: \")\n",
    "\n",
    "        else:\n",
    "            d = max(df[df[\"product\"] == prd][\"amount\"].to_list()) * 3\n",
    "            if int(amo) > d:\n",
    "                print(\"Amount yüksek abi emin misin bak !?\")\n",
    "                y_n = input(\"y / n ?\")\n",
    "                if y_n == \"y\":\n",
    "                    y = True\n",
    "                elif y_n == \"n\":\n",
    "                    amo = input(\"Amount: \")                   \n",
    "            else:\n",
    "                y = True\n",
    "    amo = int(amo)\n",
    "\n",
    "    w_days = ['pts', 'sal', 'çrş', 'prş', 'cum', 'cts', 'paz']\n",
    "    wd = str(input(\"Week day: \"))\n",
    "    z = False\n",
    "    while z == False:\n",
    "        if wd not in w_days:\n",
    "            print(\"Geçerli gün giriniz...\")\n",
    "            print(\"Geçerli günler: \", w_days)\n",
    "            wd = str(input(\"Week day: \"))\n",
    "        else:\n",
    "            z = True\n",
    "\n",
    "    typ = df[df[\"product\"] == prd][\"type\"].unique()[0] \n",
    "    comps = df[df[\"product\"] == prd][\"company\"].unique()\n",
    "    tws = df[df[\"product\"] == prd][\"town\"].unique()\n",
    "\n",
    "    df_inp = pd.DataFrame(columns = ['product', 'company', 'amount', 'town', 'type', 'week day'])\n",
    "\n",
    "    for comp in comps:\n",
    "        tw = df[(df[\"product\"] == prd) & (df[\"company\"] == comp)][\"town\"].unique()[0]\n",
    "\n",
    "        df_inp = df_inp.append({'product' : prd, 'company' : comp, 'amount' : amo, 'town' : tw,'type' : typ,\n",
    "                            'week day' : wd}, ignore_index=True)\n",
    "\n",
    "    return df_inp, comps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Input !!!!!!!!!!!!\n",
    "##### yetersiz datalardaysa ve farklı şirketlerden tedarik edilmişse ayrı ayrı ortalama süre verilebilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    #c = True\n",
    "    cond1 = False ###\n",
    "    #while c:\n",
    "    while cond1 == False: ###\n",
    "        tf, prd = is_prod_in_data(data_clean)\n",
    "        \n",
    "        if tf:\n",
    "            df_inp, comps = take_input(data_clean, prd)\n",
    "            c = False\n",
    "            cond1 = True\n",
    "        else:\n",
    "            if (is_prod_in_data_drop(drop_df, prd) == True):\n",
    "                print(\"Güvenilir sonuç için product'a ait en az 5 giriş bulunmalıdır.\")\n",
    "                print(\"\\n\",\"Daha önce bu product alımları: \")\n",
    "                a = drop_df[drop_df[\"product\"] == prd]\n",
    "                a = a.index.to_list()\n",
    "                a = data.loc[a]\n",
    "                df_inp = display(a[['company', 'amount', 'town', 'order date', 'delivery date', 'time']].style.hide(axis='index'))\n",
    "                print (\"Ortalama miktar = \", a[\"amount\"].mean(), \"Ortalama süre = \", a[\"time\"].mean())\n",
    "                comps = a[\"company\"].unique()\n",
    "                return df_inp, cond1, comps\n",
    "    \n",
    "            else:\n",
    "                df_inp = print(\"Product bulunamadı.\")\n",
    "    if cond1:\n",
    "        df_inp  = one_hot(df_inp, \"week day\")\n",
    "        df_inp  = one_hot(df_inp, \"product\")\n",
    "        df_inp  = one_hot(df_inp, \"company\")\n",
    "\n",
    "        df_inp = df_inp.drop('town',axis = 1).reset_index(drop=True)\n",
    "\n",
    "        df_inp[\"amount\"] = (df_inp[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "\n",
    "        df_inp = df_empty.append(df_inp)   # for the next version of pandas use the next line of code instead of this one. \n",
    "        #df_inp = pd.concat([df_inp, df_empty])   ------->>> for the future version of pandas\n",
    "        \n",
    "        df_inp = df_inp.fillna(0)\n",
    "        df_inp = pca.transform(df_inp)\n",
    "        \n",
    "    return df_inp, cond1, comps\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inp, cond1, comps = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cond1:\n",
    "    LR = Logistic_Regression.predict(df_inp)\n",
    "    RF = rndmForest.predict(df_inp)\n",
    "    KN = KNeighbors.predict(df_inp)\n",
    "    CSV = CSupportVector.predict(df_inp)\n",
    "    print(\"LR: \", LR, \"\\nRF:\", RF, \"\\nKN:\", KN, \"\\nCSV:\", CSV)\n",
    "    \n",
    "    res = np.array([])\n",
    "    output = np.array([])\n",
    "    for i in range(len(LR)):\n",
    "        res = np.append(res, [LR[i], RF[i], KN[i], CSV[i]])\n",
    "        m = mode(res)[0][0]\n",
    "        output = np.append(output, m)\n",
    "        res = np.array([])\n",
    "    \n",
    "    print(\"for company \", comps[i], \" predicted time = \", output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df_inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
