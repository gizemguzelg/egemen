{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9edb920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeme\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import *\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from hyperopt import tpe,hp,Trials\n",
    "from hyperopt.fmin import fmin\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41cf011a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeme\\AppData\\Local\\Temp\\ipykernel_17468\\1892989799.py:53: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data2['town'] = data2['town'].str.replace('.','missing')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing insufficient data for  company ...\n",
      "total number of removed data:  73\n",
      "persentage of removed data:  6.54 %\n",
      "removing insufficient data for  product ...\n",
      "total number of removed data:  293\n",
      "persentage of removed data:  28.09 %\n",
      "Cleaning noise ... \n",
      "deleted indices:  [228, 255, 492, 494, 532, 547, 549, 564, 663, 701, 737, 744, 787, 956, 1024, 1037, 1088, 1089]\n",
      "total number of removed data:  18\n",
      "persentage of removed data:  2.4 %\n",
      "Cleaning noise ... \n",
      "deleted indices:  [23, 34, 53, 91, 94, 103, 111, 114, 118, 135, 139, 185, 194, 195, 196, 197, 209, 218, 238, 247, 284, 286, 287, 292, 307, 308, 312, 316, 333, 336, 347, 394, 454, 458, 470, 540, 541, 552, 566, 628, 637, 655, 659, 675, 720, 728, 745, 746, 798, 807, 812, 839, 840, 851, 856, 871, 879, 882, 899, 914, 919, 947, 950, 975, 977, 981, 1005, 1035, 1065, 1066, 1082]\n",
      "total number of removed data:  71\n",
      "persentage of removed data:  9.7 %\n",
      "Mapping time\n",
      "one hot encoding  week day ...\n",
      "week day encoded.\n",
      "one hot encoding  company ...\n",
      "company encoded.\n",
      "one hot encoding  product ...\n",
      "product encoded.\n",
      "one hot encoding  order month ...\n",
      "order month encoded.\n",
      "number of features dropped from  61  to  36\n",
      "\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "zs= 0.53\n",
    "\n",
    "def accuracy_calculate(actual_values, predicted_values):\n",
    "    comparison = abs(np.round(predicted_values) - actual_values)\n",
    "    accuracy = 1- ((len(comparison[comparison>=(0+1)])) / len(actual_values))\n",
    "    return accuracy\n",
    "\n",
    "def mean_iou_calculator(actual_values, predicted_values, time):\n",
    "    confusion_array = confusion_matrix(actual_values, predicted_values)\n",
    "    individual_ious = []\n",
    "    for i in range(len(confusion_array)):\n",
    "        individual_iou = confusion_array[i][i] / (sum(confusion_array[i]))\n",
    "        individual_ious.append(individual_iou)\n",
    "    mean_iou = sum(individual_ious)/len(individual_ious)\n",
    "    results = pd.DataFrame()  \n",
    "    featue_y_values = sorted(data[time].unique())\n",
    "    for i in range(len(individual_ious)):\n",
    "        results.insert(0, 'iou_(' + str(featue_y_values[i])  +')', [individual_ious[i]], True)\n",
    "    results = results[results.columns[::-1]]\n",
    "    results.insert(0, 'mean_iou', mean_iou, True)\n",
    "    return results\n",
    "\n",
    "data = pd.read_csv('gulle.csv', sep=';', encoding = \"utf8\")\n",
    "#Rearannge the dataframe as the old one\n",
    "data.columns = ['product', 'amount', 'company', 'town', 'type', 'order date', 'delivery date', 'time']\n",
    "\n",
    "data2 = data.drop('delivery date', axis = 1)\n",
    "data2[\"order day\"] = ''\n",
    "data2[\"order month\"] = ''\n",
    "data2[\"week day\"] = ''\n",
    "\n",
    "\n",
    "# Remove 'order date' and add 'order day', 'order month' and 'week day' features\n",
    "for i in range(len(data2)):\n",
    "  data2.at[i, 'order day'] = data2['order date'][i].split()[0]\n",
    "  data2.at[i, 'order month'] = data2['order date'][i].split()[1]\n",
    "  data2.at[i, 'week day'] = data2['order date'][i].split()[-1]\n",
    "data2 = data2.drop('order date', axis = 1)\n",
    "data2['week day'] = data2['week day'].str.replace('Pazartesi','pts')\n",
    "data2['week day'] = data2['week day'].str.replace('Salı','sal')\n",
    "data2['week day'] = data2['week day'].str.replace('Çarşamba','çrş')\n",
    "data2['week day'] = data2['week day'].str.replace('Perşembe','prş')\n",
    "data2['week day'] = data2['week day'].str.replace('Cumartesi','cts')\n",
    "data2['week day'] = data2['week day'].str.replace('Cuma','cum')\n",
    "data2['week day'] = data2['week day'].str.replace('Pazar','paz')\n",
    "\n",
    "# data2 = data2[data2[\"week day\"].str.contains(\"Pazar\") == False]\n",
    "\n",
    "# Rearranging Dataframe\n",
    "data2 = data2[['product', 'company', 'amount', 'town', 'type', 'order day', 'week day', 'order month', 'time']]\n",
    "data2['town'] = data2['town'].str.lower()\n",
    "data2['town'] = data2['town'].str.replace('i̇','i')\n",
    "data2['town'] = data2['town'].str.replace('.','missing')\n",
    "data2['town'] = data2['town'].str.replace(' tekirdağ','tekirdağ')\n",
    "data2['town'] = data2['town'].str.replace('küçükçekmece','istanbul')\n",
    "data2['town'] = data2['town'].str.replace('çorlu','tekirdağ')\n",
    "data2['town'] = data2['town'].str.replace('bandirma','balıkesir')\n",
    "\n",
    "#data2 = data2.drop('town',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.drop('order day',axis = 1).reset_index(drop=True)\n",
    "#data2 = data2.drop('order month',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.fillna(\"missing\")\n",
    "#data2 = data2[data2[\"town\"].str.contains(\"missing\") == False]\n",
    "\n",
    "data_clean = data2.copy()\n",
    "drop_df = data2.copy()\n",
    "drop_index_list = []\n",
    "\n",
    "\n",
    "########################################################################################### REMOVE INSUFFICIENT DATA\n",
    "def remove_insuff(df, ft):\n",
    "    print(\"removing insufficient data for \", ft, \"...\")\n",
    "    fst_len = len(df)\n",
    "    x = df[ft].value_counts() < 10 \n",
    "    df2 = x.to_frame().reset_index()\n",
    "    df2.columns = [ft, 'booly']\n",
    "    df2.drop(df2[df2.booly == False].index, inplace=True)\n",
    "    drop_list = df2[ft].tolist()\n",
    "    drop_indices=[]\n",
    "\n",
    "    if len(drop_list) != 0:\n",
    "        for i in df.index:\n",
    "            for j in range(len(drop_list)):\n",
    "                if (drop_list[j] == df.at[i, ft]):\n",
    "                    drop_indices = drop_indices + [i]\n",
    "        df.drop(drop_indices, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        drop_indices = []\n",
    "                        \n",
    "    lst_len = len(df)\n",
    "    rem = fst_len - lst_len      # number of removed data\n",
    "    per = (rem / fst_len) * 100  # percentage of removed data\n",
    "\n",
    "    print(\"total number of removed data: \", rem)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    return df, drop_indices\n",
    "\n",
    "data2, drop_indices = remove_insuff(data2, \"company\")\n",
    "drop_index_list = drop_index_list + drop_indices\n",
    "\n",
    "data2, drop_indices = remove_insuff(data2, \"product\")\n",
    "drop_index_list = drop_index_list + drop_indices\n",
    "\n",
    "\n",
    "########################################################################################### CLEAN NOISE\n",
    "def clean_noise(df): \n",
    "    in_len = len(df)\n",
    "    zs = 0.89\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "    \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                max_min_a = df_max_scaled2[\"amount\"].max() - df_max_scaled2[\"amount\"].min()\n",
    "                \n",
    "                if (max_min_a != 0) and (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zs\n",
    "\n",
    "                    df_max_scaled2[\"amount\"] = (df_max_scaled2[\"amount\"] - df_max_scaled2[\"amount\"].min()) / max_min_a\n",
    "                    amo_sc = df_max_scaled2[\"amount\"]\n",
    "                    df_zscore_a = (amo_sc - amo_sc.mean())/amo_sc.std()\n",
    "                    dfz_a = abs(df_zscore_a) > zs\n",
    "\n",
    "                    df1 = dfz_t[\"time\"] & dfz_a \n",
    "                    df2 = (df_zscore_t[\"time\"] * df_zscore_a) < 0 \n",
    "                    dfz = df1 & df2 \n",
    "\n",
    "                    index_drop_list = index_drop_list + [*filter(dfz.get, dfz.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list\n",
    "\n",
    "def clean_noise_tt(df): \n",
    "    in_len = len(df)\n",
    "    zst = 1.2\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "        \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                \n",
    "                if (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zst\n",
    "                    \n",
    "                    index_drop_list += dfz_t[dfz_t[\"time\"].eq(True)].index.tolist()\n",
    "                    \n",
    "                    #index_drop_list = index_drop_list + [*filter(dfz_t.get, dfz_t.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "data2, train_drop_list = clean_noise(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list\n",
    "\n",
    "data2, train_drop_list = clean_noise_tt(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list\n",
    "\n",
    "\n",
    "########################################################################################### GROUP TIME\n",
    "def group_time(df):\n",
    "    \n",
    "    min_samp = len(df)*0.1 #len(df)*0.07\n",
    "    t = 1\n",
    "    gs = 0\n",
    "    group = []\n",
    "    sub_group = [t]\n",
    "    #gap = df[\"time\"].std()\n",
    "\n",
    "    while t <= df[\"time\"].max():\n",
    "        \n",
    "        \n",
    "        if sum(df[\"time\"] == t) > 0:\n",
    "        \n",
    "            gs += sum(df[\"time\"] == t)\n",
    "\n",
    "            if (len(sub_group) > 0) and ((t - min(sub_group)) <= math.ceil(math.sqrt(min(sub_group)))):\n",
    "                sub_group += [t]\n",
    "\n",
    "            else:\n",
    "                if (len(sub_group) != 0):\n",
    "                    gs = 0\n",
    "                    group += [sub_group]\n",
    "                sub_group = [t]\n",
    "\n",
    "            if (gs >= min_samp) or ((t - min(sub_group)) > math.ceil(math.sqrt(min(sub_group)))):\n",
    "                gs = 0\n",
    "                group += [sub_group]\n",
    "                sub_group = []\n",
    "                \n",
    "        if t == df[\"time\"].max() :\n",
    "            group += [sub_group]\n",
    "            group[0].remove(1)\n",
    "            \n",
    "            c = True               # son sub_grouptan bir önceki sub_groupa time aktarımı\n",
    "            gap = group[-2][0]     # son sub_grouptan bir önceki sub_groupa yollanan time değerleri arasındaki maksimum fark \n",
    "            while c:\n",
    "                if len(group[- 1]) >= 2:\n",
    "                    x = group[-1][0]\n",
    "                    x_l = group[-2] [-1]\n",
    "                    x_r = group[-1] [1]\n",
    "\n",
    "                    if (x - group[-2][0]) <= math.ceil(math.sqrt(group[-2][0])):\n",
    "                        group[-2].append(x)\n",
    "                        group[-1].remove(x)\n",
    "                \n",
    "                x = group[-1][0]\n",
    "                if (x - group[-2][0]) <= math.ceil(math.sqrt(group[-2][0])):\n",
    "                    group[-2].append(x)\n",
    "                    group[-1].remove(x)\n",
    "                    \n",
    "                else:\n",
    "                    c = False\n",
    "            \n",
    "            c = True     # son sub_groupta bulunan zamanlar dataframede 5 kereden az bulunuyorsa bu sub_groupu sil\n",
    "            insuff_time = []\n",
    "            while c:\n",
    "                s = 0\n",
    "                for i in range(len(group[-1])):\n",
    "                    s += sum(df[\"time\"] == group[-1][i])\n",
    "                if s < 5:\n",
    "                    insuff_time += group.pop(-1)\n",
    "                else:\n",
    "                    c = False\n",
    "\n",
    "        t += 1\n",
    "        \n",
    "    return group, insuff_time\n",
    "\n",
    "groups, insuff_time = group_time(data2)\n",
    "\n",
    "\n",
    "########################################################################################### MAP TIME\n",
    "def map_time(df, groups, insuff_time):\n",
    "    print(\"Mapping time\")\n",
    "    index_drop_list = []\n",
    "    for i in df.index:\n",
    "        for t in insuff_time:\n",
    "            if df.at[i, \"time\"] == t:\n",
    "                index_drop_list += [i]\n",
    "                \n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "\n",
    "    time = 0\n",
    "    for sub_group in groups:\n",
    "        time += 1\n",
    "        for t in sub_group:\n",
    "            for i in df.index:\n",
    "                if df.at[i, \"time\"] == t:\n",
    "                    df.at[i, \"time\"] = time\n",
    "    return df\n",
    "            \n",
    "map_time(data2, groups, insuff_time)\n",
    "\n",
    "\n",
    "########################################################################################### ONE HOT ENCODING\n",
    "def one_hot(df, ft):      ### ft = \"company\", \"product\", \"week day\" etc.\n",
    "    print(\"one hot encoding \", ft, \"...\")\n",
    "    dum = pd.get_dummies(df[ft])\n",
    "    df = df.drop(ft, axis = 1)\n",
    "    df = df.join(dum)\n",
    "    print(ft, \"encoded.\")\n",
    "    return df\n",
    "\n",
    "data2  = one_hot(data2, \"week day\")\n",
    "data2  = one_hot(data2, \"company\")\n",
    "data2  = one_hot(data2, \"product\")\n",
    "data2  = one_hot(data2, \"order month\")\n",
    "\n",
    "\n",
    "########################################################################################### DROP_DF & DATA_CLEAN\n",
    "drop_index_list = sorted(list(set(drop_index_list)))\n",
    "drop_df = drop_df.loc[drop_index_list]\n",
    "\n",
    "data_clean.drop(drop_df.index.to_list(), axis=0, inplace=True)\n",
    "\n",
    "\n",
    "########################################################################################### DROP TOWN COLUMN\n",
    "data2 = data2.drop('town',axis = 1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "########################################################################################### TRAIN-TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data2.copy()\n",
    "Y = data2.copy()\n",
    "X.drop(\"time\", axis=1, inplace=True)\n",
    "Y = Y[\"time\"]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "########################################################################################### NORMALIZE AMOUNT\n",
    "xt_min = x_train[\"amount\"].min()\n",
    "xt_max = x_train[\"amount\"].max()\n",
    "\n",
    "x_train[\"amount\"] = (x_train[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "x_val[\"amount\"] = (x_val[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "\n",
    "\n",
    "########################################################################################### DF_EMPTY\n",
    "df_empty = x_train[0:0]\n",
    "\n",
    "\n",
    "########################################################################################### PCA\n",
    "def do_pca(x_train, x_val):\n",
    "    from sklearn.decomposition import PCA\n",
    "    xl = len(x_train.columns)\n",
    "    pca = PCA(.95)\n",
    "    pca.fit(x_train)\n",
    "    print(\"number of features dropped from \", xl, \" to \", pca.n_components_) \n",
    "    #print(\"variance ratio: \", pca.explained_variance_ratio_) \n",
    "\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_val = pca.transform(x_val)\n",
    "    return pca, x_train, x_val\n",
    "\n",
    "pca, x_train, x_val = do_pca(x_train, x_val)\n",
    "\n",
    "print(\"\\ndone...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c55cf09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4, 5, 6], [7, 9, 10]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "406c62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b143fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 700/700 [13:55<00:00,  1.19s/trial, best loss: 1.4617834394904459]\n"
     ]
    }
   ],
   "source": [
    "seed=42\n",
    "def objective(params):\n",
    "    est = int(params['n_estimators'])\n",
    "    md = int(params['max_depth'])\n",
    "    msl = int(params['min_samples_leaf'])\n",
    "    mss = int(params['min_samples_split'])\n",
    "    clf = RandomForestClassifier(random_state=42,n_estimators=est,max_depth=md,min_samples_leaf=msl,min_samples_split=mss, n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    #score = cross_val_score(clf, X_train, y_train, cv=3, scoring='balanced_accuracy').mean()\n",
    "    score = cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "    return 1/score\n",
    "\n",
    "def optimize(trial):\n",
    "    params={'n_estimators':hp.uniform('n_estimators',100,1000),\n",
    "           'max_depth':hp.uniform('max_depth',5,30),\n",
    "           'min_samples_leaf':hp.uniform('min_samples_leaf',1,10),\n",
    "           'min_samples_split':hp.uniform('min_samples_split',2,10)}\n",
    "    best = fmin(fn=objective,space=params,algo=tpe.suggest,trials=trial,max_evals=700,rstate=np.random.default_rng(seed))\n",
    "    return best\n",
    "\n",
    "trial=Trials()\n",
    "best=optimize(trial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3d88910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9.218835627017297,\n",
       " 'min_samples_leaf': 1.9200962753481647,\n",
       " 'min_samples_split': 2.427809432079914,\n",
       " 'n_estimators': 276.03003906073906}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c615b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best[\"max_depth\"] = round(best[\"max_depth\"])\n",
    "best[\"min_samples_leaf\"] = round(best[\"min_samples_leaf\"])\n",
    "best[\"min_samples_split\"] = round(best[\"min_samples_split\"])\n",
    "best[\"n_estimators\"] = round(best[\"n_estimators\"])\n",
    "\n",
    "params = best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ec02d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "435e0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    b_accuracy = balanced_accuracy_score(y_test,predictions)\n",
    "    accuracy = accuracy_score(y_test,predictions)\n",
    "    print('Model Performance')\n",
    "    print('Balanced accuracy = {:0.2f}%.'.format(b_accuracy))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4bfb1411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForest classifier:  0.6470588235294118\n",
      "Accuracy of RandomForest_opt classifier:  0.7016806722689075\n"
     ]
    }
   ],
   "source": [
    "rndmForest = RandomForestClassifier(random_state=42).fit(x_train, y_train)\n",
    "rndmForest_opt = RandomForestClassifier(random_state=42, **params).fit(x_train, y_train)\n",
    "\n",
    "rndmForest_predictions = rndmForest.predict(x_val)\n",
    "rndmForest_opt_predictions = rndmForest_opt.predict(x_val)\n",
    "\n",
    "print('Accuracy of RandomForest classifier: ', accuracy_score(y_val, rndmForest_predictions))\n",
    "print('Balanced Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_val, rndmForest_predictions))\n",
    "print('Accuracy of RandomForest_opt classifier: ', accuracy_score(y_val, rndmForest_opt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd547d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1591490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
