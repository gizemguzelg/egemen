{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df4e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeme\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import *\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from hyperopt import tpe,hp,Trials\n",
    "from hyperopt.fmin import fmin\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fe4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeme\\AppData\\Local\\Temp\\ipykernel_14648\\1665427725.py:53: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data2['town'] = data2['town'].str.replace('.','missing')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing insufficient data for  company ...\n",
      "total number of removed data:  73\n",
      "persentage of removed data:  6.54 %\n",
      "removing insufficient data for  product ...\n",
      "total number of removed data:  293\n",
      "persentage of removed data:  28.09 %\n",
      "Cleaning noise ... \n",
      "deleted indices:  [228, 255, 492, 494, 532, 547, 549, 564, 663, 701, 737, 744, 787, 956, 1024, 1037, 1088, 1089]\n",
      "total number of removed data:  18\n",
      "persentage of removed data:  2.4 %\n",
      "Cleaning noise ... \n",
      "deleted indices:  [23, 34, 53, 91, 94, 103, 111, 114, 118, 135, 139, 185, 194, 195, 196, 197, 209, 218, 238, 247, 284, 286, 287, 292, 307, 308, 312, 316, 333, 336, 347, 394, 454, 458, 470, 540, 541, 552, 566, 628, 637, 655, 659, 675, 720, 728, 745, 746, 798, 807, 812, 839, 840, 851, 856, 871, 879, 882, 899, 914, 919, 947, 950, 975, 977, 981, 1005, 1035, 1065, 1066, 1082]\n",
      "total number of removed data:  71\n",
      "persentage of removed data:  9.7 %\n",
      "Mapping time\n",
      "one hot encoding  week day ...\n",
      "week day encoded.\n",
      "one hot encoding  company ...\n",
      "company encoded.\n",
      "one hot encoding  product ...\n",
      "product encoded.\n",
      "one hot encoding  order month ...\n",
      "order month encoded.\n",
      "number of features dropped from  61  to  2\n",
      "\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "zs= 0.53\n",
    "\n",
    "def accuracy_calculate(actual_values, predicted_values):\n",
    "    comparison = abs(np.round(predicted_values) - actual_values)\n",
    "    accuracy = 1- ((len(comparison[comparison>=(0+1)])) / len(actual_values))\n",
    "    return accuracy\n",
    "\n",
    "def mean_iou_calculator(actual_values, predicted_values, time):\n",
    "    confusion_array = confusion_matrix(actual_values, predicted_values)\n",
    "    individual_ious = []\n",
    "    for i in range(len(confusion_array)):\n",
    "        individual_iou = confusion_array[i][i] / (sum(confusion_array[i]))\n",
    "        individual_ious.append(individual_iou)\n",
    "    mean_iou = sum(individual_ious)/len(individual_ious)\n",
    "    results = pd.DataFrame()  \n",
    "    featue_y_values = sorted(data[time].unique())\n",
    "    for i in range(len(individual_ious)):\n",
    "        results.insert(0, 'iou_(' + str(featue_y_values[i])  +')', [individual_ious[i]], True)\n",
    "    results = results[results.columns[::-1]]\n",
    "    results.insert(0, 'mean_iou', mean_iou, True)\n",
    "    return results\n",
    "\n",
    "data = pd.read_csv('gulle.csv', sep=';', encoding = \"utf8\")\n",
    "#Rearannge the dataframe as the old one\n",
    "data.columns = ['product', 'amount', 'company', 'town', 'type', 'order date', 'delivery date', 'time']\n",
    "\n",
    "data2 = data.drop('delivery date', axis = 1)\n",
    "data2[\"order day\"] = ''\n",
    "data2[\"order month\"] = ''\n",
    "data2[\"week day\"] = ''\n",
    "\n",
    "\n",
    "# Remove 'order date' and add 'order day', 'order month' and 'week day' features\n",
    "for i in range(len(data2)):\n",
    "  data2.at[i, 'order day'] = data2['order date'][i].split()[0]\n",
    "  data2.at[i, 'order month'] = data2['order date'][i].split()[1]\n",
    "  data2.at[i, 'week day'] = data2['order date'][i].split()[-1]\n",
    "data2 = data2.drop('order date', axis = 1)\n",
    "data2['week day'] = data2['week day'].str.replace('Pazartesi','pts')\n",
    "data2['week day'] = data2['week day'].str.replace('Salı','sal')\n",
    "data2['week day'] = data2['week day'].str.replace('Çarşamba','çrş')\n",
    "data2['week day'] = data2['week day'].str.replace('Perşembe','prş')\n",
    "data2['week day'] = data2['week day'].str.replace('Cumartesi','cts')\n",
    "data2['week day'] = data2['week day'].str.replace('Cuma','cum')\n",
    "data2['week day'] = data2['week day'].str.replace('Pazar','paz')\n",
    "\n",
    "# data2 = data2[data2[\"week day\"].str.contains(\"Pazar\") == False]\n",
    "\n",
    "# Rearranging Dataframe\n",
    "data2 = data2[['product', 'company', 'amount', 'town', 'type', 'order day', 'week day', 'order month', 'time']]\n",
    "data2['town'] = data2['town'].str.lower()\n",
    "data2['town'] = data2['town'].str.replace('i̇','i')\n",
    "data2['town'] = data2['town'].str.replace('.','missing')\n",
    "data2['town'] = data2['town'].str.replace(' tekirdağ','tekirdağ')\n",
    "data2['town'] = data2['town'].str.replace('küçükçekmece','istanbul')\n",
    "data2['town'] = data2['town'].str.replace('çorlu','tekirdağ')\n",
    "data2['town'] = data2['town'].str.replace('bandirma','balıkesir')\n",
    "\n",
    "#data2 = data2.drop('town',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.drop('order day',axis = 1).reset_index(drop=True)\n",
    "#data2 = data2.drop('order month',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.fillna(\"missing\")\n",
    "#data2 = data2[data2[\"town\"].str.contains(\"missing\") == False]\n",
    "\n",
    "data_clean = data2.copy()\n",
    "drop_df = data2.copy()\n",
    "drop_index_list = []\n",
    "\n",
    "\n",
    "########################################################################################### REMOVE INSUFFICIENT DATA\n",
    "def remove_insuff(df, ft):\n",
    "    print(\"removing insufficient data for \", ft, \"...\")\n",
    "    fst_len = len(df)\n",
    "    x = df[ft].value_counts() < 10\n",
    "    df2 = x.to_frame().reset_index()\n",
    "    df2.columns = [ft, 'booly']\n",
    "    df2.drop(df2[df2.booly == False].index, inplace=True)\n",
    "    drop_list = df2[ft].tolist()\n",
    "    drop_indices=[]\n",
    "\n",
    "    if len(drop_list) != 0:\n",
    "        for i in df.index:\n",
    "            for j in range(len(drop_list)):\n",
    "                if (drop_list[j] == df.at[i, ft]):\n",
    "                    drop_indices = drop_indices + [i]\n",
    "        df.drop(drop_indices, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        drop_indices = []\n",
    "                        \n",
    "    lst_len = len(df)\n",
    "    rem = fst_len - lst_len      # number of removed data\n",
    "    per = (rem / fst_len) * 100  # percentage of removed data\n",
    "\n",
    "    print(\"total number of removed data: \", rem)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    return df, drop_indices\n",
    "\n",
    "data2, drop_indices = remove_insuff(data2, \"company\")\n",
    "drop_index_list = drop_index_list + drop_indices\n",
    "\n",
    "data2, drop_indices = remove_insuff(data2, \"product\")\n",
    "drop_index_list = drop_index_list + drop_indices\n",
    "\n",
    "\n",
    "########################################################################################### CLEAN NOISE\n",
    "def clean_noise(df): \n",
    "    in_len = len(df)\n",
    "    zs = 0.89\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "    \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                max_min_a = df_max_scaled2[\"amount\"].max() - df_max_scaled2[\"amount\"].min()\n",
    "                \n",
    "                if (max_min_a != 0) and (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zs\n",
    "\n",
    "                    df_max_scaled2[\"amount\"] = (df_max_scaled2[\"amount\"] - df_max_scaled2[\"amount\"].min()) / max_min_a\n",
    "                    amo_sc = df_max_scaled2[\"amount\"]\n",
    "                    df_zscore_a = (amo_sc - amo_sc.mean())/amo_sc.std()\n",
    "                    dfz_a = abs(df_zscore_a) > zs\n",
    "\n",
    "                    df1 = dfz_t[\"time\"] & dfz_a \n",
    "                    df2 = (df_zscore_t[\"time\"] * df_zscore_a) < 0 \n",
    "                    dfz = df1 & df2 \n",
    "\n",
    "                    index_drop_list = index_drop_list + [*filter(dfz.get, dfz.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list\n",
    "\n",
    "def clean_noise_tt(df): \n",
    "    in_len = len(df)\n",
    "    zst = 1.2\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "        \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                \n",
    "                if (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zst\n",
    "                    \n",
    "                    index_drop_list += dfz_t[dfz_t[\"time\"].eq(True)].index.tolist()\n",
    "                    \n",
    "                    #index_drop_list = index_drop_list + [*filter(dfz_t.get, dfz_t.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "data2, train_drop_list = clean_noise(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list\n",
    "\n",
    "data2, train_drop_list = clean_noise_tt(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list\n",
    "\n",
    "\n",
    "########################################################################################### GROUP TIME\n",
    "def group_time(df):\n",
    "    \n",
    "    min_samp = len(df)*0.07 #len(df)*0.07\n",
    "    t = 1\n",
    "    gs = 0\n",
    "    group = []\n",
    "    sub_group = [t]\n",
    "    #gap = df[\"time\"].std()\n",
    "\n",
    "    while t <= df[\"time\"].max():\n",
    "        \n",
    "        \n",
    "        if sum(df[\"time\"] == t) > 0:\n",
    "        \n",
    "            gs += sum(df[\"time\"] == t)\n",
    "\n",
    "            if (len(sub_group) > 0) and ((t - min(sub_group)) <= math.ceil(math.sqrt(min(sub_group)))):\n",
    "                sub_group += [t]\n",
    "\n",
    "            else:\n",
    "                if (len(sub_group) != 0):\n",
    "                    gs = 0\n",
    "                    group += [sub_group]\n",
    "                sub_group = [t]\n",
    "\n",
    "            if (gs >= min_samp) or ((t - min(sub_group)) > math.ceil(math.sqrt(min(sub_group)))):\n",
    "                gs = 0\n",
    "                group += [sub_group]\n",
    "                sub_group = []\n",
    "                \n",
    "        if t == df[\"time\"].max() :\n",
    "            group += [sub_group]\n",
    "            group[0].remove(1)\n",
    "            \n",
    "            c = True               # son sub_grouptan bir önceki sub_groupa time aktarımı\n",
    "            gap = group[-2][0]     # son sub_grouptan bir önceki sub_groupa yollanan time değerleri arasındaki maksimum fark \n",
    "            while c:\n",
    "                if len(group[- 1]) >= 2:\n",
    "                    x = group[-1][0]\n",
    "                    x_l = group[-2] [-1]\n",
    "                    x_r = group[-1] [1]\n",
    "\n",
    "                    if (x - group[-2][0]) <= math.ceil(math.sqrt(group[-2][0])):\n",
    "                        group[-2].append(x)\n",
    "                        group[-1].remove(x)\n",
    "                \n",
    "                x = group[-1][0]\n",
    "                if (x - group[-2][0]) <= math.ceil(math.sqrt(group[-2][0])):\n",
    "                    group[-2].append(x)\n",
    "                    group[-1].remove(x)\n",
    "                    \n",
    "                else:\n",
    "                    c = False\n",
    "            \n",
    "            c = True     # son sub_groupta bulunan zamanlar dataframede 5 kereden az bulunuyorsa bu sub_groupu sil\n",
    "            insuff_time = []\n",
    "            while c:\n",
    "                s = 0\n",
    "                for i in range(len(group[-1])):\n",
    "                    s += sum(df[\"time\"] == group[-1][i])\n",
    "                if s < 5:\n",
    "                    insuff_time += group.pop(-1)\n",
    "                else:\n",
    "                    c = False\n",
    "\n",
    "        t += 1\n",
    "        \n",
    "    return group, insuff_time\n",
    "\n",
    "groups, insuff_time = group_time(data2)\n",
    "\n",
    "\n",
    "########################################################################################### MAP TIME\n",
    "def map_time(df, groups, insuff_time):\n",
    "    print(\"Mapping time\")\n",
    "    index_drop_list = []\n",
    "    for i in df.index:\n",
    "        for t in insuff_time:\n",
    "            if df.at[i, \"time\"] == t:\n",
    "                index_drop_list += [i]\n",
    "                \n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "\n",
    "    time = 0\n",
    "    for sub_group in groups:\n",
    "        time += 1\n",
    "        for t in sub_group:\n",
    "            for i in df.index:\n",
    "                if df.at[i, \"time\"] == t:\n",
    "                    df.at[i, \"time\"] = time\n",
    "    return df\n",
    "            \n",
    "map_time(data2, groups, insuff_time)\n",
    "\n",
    "\n",
    "########################################################################################### ONE HOT ENCODING\n",
    "def one_hot(df, ft):      ### ft = \"company\", \"product\", \"week day\" etc.\n",
    "    print(\"one hot encoding \", ft, \"...\")\n",
    "    dum = pd.get_dummies(df[ft])\n",
    "    df = df.drop(ft, axis = 1)\n",
    "    df = df.join(dum)\n",
    "    print(ft, \"encoded.\")\n",
    "    return df\n",
    "\n",
    "data2  = one_hot(data2, \"week day\")\n",
    "data2  = one_hot(data2, \"company\")\n",
    "data2  = one_hot(data2, \"product\")\n",
    "data2  = one_hot(data2, \"order month\")\n",
    "\n",
    "\n",
    "########################################################################################### DROP_DF & DATA_CLEAN\n",
    "drop_index_list = sorted(list(set(drop_index_list)))\n",
    "drop_df = drop_df.loc[drop_index_list]\n",
    "\n",
    "data_clean.drop(drop_df.index.to_list(), axis=0, inplace=True)\n",
    "\n",
    "\n",
    "########################################################################################### DROP TOWN COLUMN\n",
    "data2 = data2.drop('town',axis = 1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "########################################################################################### TRAIN-TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data2.copy()\n",
    "Y = data2.copy()\n",
    "X.drop(\"time\", axis=1, inplace=True)\n",
    "Y = Y[\"time\"]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "########################################################################################### NORMALIZE AMOUNT\n",
    "xt_min = x_train[\"amount\"].min()\n",
    "xt_max = x_train[\"amount\"].max()\n",
    "\n",
    "x_train[\"amount\"] = (x_train[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "x_val[\"amount\"] = (x_val[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "\n",
    "\n",
    "########################################################################################### DF_EMPTY\n",
    "df_empty = x_train[0:0]\n",
    "\n",
    "\n",
    "########################################################################################### PCA\n",
    "def do_pca(x_train, x_val):\n",
    "    from sklearn.decomposition import PCA\n",
    "    xl = len(x_train.columns)\n",
    "    pca = PCA(2)\n",
    "    pca.fit(x_train)\n",
    "    print(\"number of features dropped from \", xl, \" to \", pca.n_components_) \n",
    "    #print(\"variance ratio: \", pca.explained_variance_ratio_) \n",
    "\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_val = pca.transform(x_val)\n",
    "    return pca, x_train, x_val\n",
    "\n",
    "pca, x_train, x_val = do_pca(x_train, x_val)\n",
    "\n",
    "print(\"\\ndone...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afc529ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84d50e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_qubits:  2\n",
      "aaa\n",
      "bbb\n",
      "[[1.         0.72664455 0.83166245 ... 0.61427323 0.86546441 0.80870765]\n",
      " [0.72664455 1.         0.85067939 ... 0.90681799 0.85595887 0.85662117]\n",
      " [0.83166245 0.85067939 1.         ... 0.75421242 0.76655244 0.73367125]\n",
      " ...\n",
      " [0.61427323 0.90681799 0.75421242 ... 1.         0.76080242 0.7993527 ]\n",
      " [0.86546441 0.85595887 0.76655244 ... 0.76080242 1.         0.98860272]\n",
      " [0.80870765 0.85662117 0.73367125 ... 0.7993527  0.98860272 1.        ]]\n",
      "[[0.99911762 0.72774337 0.82007827 ... 0.60957483 0.87528038 0.81724315]\n",
      " [0.83444071 0.8657492  0.75565433 ... 0.78870886 0.99671821 0.99710237]\n",
      " [0.99935004 0.714022   0.82966332 ... 0.60674919 0.85009081 0.79353041]\n",
      " ...\n",
      " [0.9939386  0.67834106 0.80744114 ... 0.57799296 0.81545957 0.75800502]\n",
      " [0.6682938  0.69916612 0.79421284 ... 0.54467628 0.56868431 0.51825218]\n",
      " [0.83854956 0.85341971 0.99984276 ... 0.75564035 0.77437855 0.74132034]]\n",
      "acc test: 0.6091370558375635\n",
      "[[1.         0.72664455 0.83166245 ... 0.61427323 0.86546441 0.80870765]\n",
      " [0.72664455 1.         0.85067939 ... 0.90681799 0.85595887 0.85662117]\n",
      " [0.83166245 0.85067939 1.         ... 0.75421242 0.76655244 0.73367125]\n",
      " ...\n",
      " [0.61427323 0.90681799 0.75421242 ... 1.         0.76080242 0.7993527 ]\n",
      " [0.86546441 0.85595887 0.76655244 ... 0.76080242 1.         0.98860272]\n",
      " [0.80870765 0.85662117 0.73367125 ... 0.7993527  0.98860272 1.        ]]\n",
      "acc train: 0.6004366812227074\n",
      "data, accuracy, precision, recall, f1 score\n",
      "[['test', 0.6091370558375635], ['train', 0.6004366812227074], datetime.timedelta(seconds=993, microseconds=381691)]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "import csv\n",
    "import os\n",
    "#os.environ['OMP_NUM_THREADS'] = \"10\"\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pennylane as qml\n",
    "from pennylane.templates import IQPEmbedding\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "n_qubits = len(X_train[0])\n",
    "print(\"n_qubits: \",n_qubits)\n",
    "\n",
    "dev_kernel = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "projector = np.zeros((2**n_qubits, 2**n_qubits))\n",
    "projector[0, 0] = 1\n",
    "\n",
    "@qml.qnode(dev_kernel)\n",
    "def kernel(x1, x2):\n",
    "    IQPEmbedding(x1, wires=range(n_qubits), n_repeats=2)\n",
    "    qml.adjoint(IQPEmbedding(x2, wires=range(n_qubits), n_repeats=2))\n",
    "    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))\n",
    "\n",
    "def kernel_matrix(A, B):\n",
    "    if len(A)==len(B):\n",
    "        x=np.zeros((len(A),len(A)))\n",
    "        for i in range(len(A)):\n",
    "            for j in range(i+1):\n",
    "                if(i==j):\n",
    "                    x[i][j]=1\n",
    "                else:                    \n",
    "                    x[i][j]= kernel(A[i],A[j])\n",
    "                    x[j][i]= x[i][j]\n",
    "                    \n",
    "    else:\n",
    "        x=np.array([[kernel(a, b) for b in B] for a in A])\n",
    "    print (x)\n",
    "    return x\n",
    "\n",
    "\n",
    "print(\"aaa\")\n",
    "#tr_te = kernel_matrix(X_train, X_test)\n",
    "print(\"bbb\")\n",
    "\n",
    "svm = SVC(kernel=kernel_matrix).fit(X_train, y_train)\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "print(\"acc test:\", accuracy_score(predictions, y_test))\n",
    "\n",
    "results_test=['test']\n",
    "results_test.append(accuracy_score(predictions, y_test))\n",
    "#results_test.append(precision_score(predictions, y_test))\n",
    "#results_test.append(recall_score(predictions, y_test))\n",
    "#results_test.append(f1_score(predictions, y_test))\n",
    "\n",
    "predictions = svm.predict(X_train)\n",
    "print(\"acc train:\", accuracy_score(predictions, y_train))\n",
    "\n",
    "results_train=['train']\n",
    "results_train.append(accuracy_score(predictions, y_train))\n",
    "#results_train.append(precision_score(predictions, y_train))                     \n",
    "#results_train.append(recall_score(predictions, y_train))\n",
    "#results_train.append(f1_score(predictions, y_train))\n",
    "\n",
    "end_time = datetime.now()\n",
    "now = (end_time - start_time)\n",
    "time = ['time', now]\n",
    "fields = ['data','accuracy', 'precision', 'recall', 'f1'] \n",
    "\n",
    "with open('iqp2rep', 'w') as f:\n",
    "      \n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerow(results_test)\n",
    "    write.writerow(results_train)  \n",
    "    write.writerow(time)\n",
    "    \n",
    "print(\"data, accuracy, precision, recall, f1 score\")\n",
    "results = [results_test,results_train,now]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55b117af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel=<function kernel_matrix at 0x00000273464593A0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0eb8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_te = kernel_matrix(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0bda212",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = kernel_matrix(X_train, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6b7ac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99911762, 0.72774337, 0.82007827, ..., 0.60957483, 0.87528038,\n",
       "        0.81724315],\n",
       "       [0.83444071, 0.8657492 , 0.75565433, ..., 0.78870886, 0.99671821,\n",
       "        0.99710237],\n",
       "       [0.99935004, 0.714022  , 0.82966332, ..., 0.60674919, 0.85009081,\n",
       "        0.79353041],\n",
       "       ...,\n",
       "       [0.9939386 , 0.67834106, 0.80744114, ..., 0.57799296, 0.81545957,\n",
       "        0.75800502],\n",
       "       [0.6682938 , 0.69916612, 0.79421284, ..., 0.54467628, 0.56868431,\n",
       "        0.51825218],\n",
       "       [0.83854956, 0.85341971, 0.99984276, ..., 0.75564035, 0.77437855,\n",
       "        0.74132034]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d971ad44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': <function __main__.kernel_matrix(A, B)>,\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12a5b364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52c6ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tr_te.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38725088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeme\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:214: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self.kernel == \"precomputed\" and n_samples != X.shape[1]:\n",
      "C:\\Users\\egeme\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:231: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if kernel == \"precomputed\":\n",
      "C:\\Users\\egeme\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:315: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  ) = libsvm.fit(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array([[1.        , 0.72664455, 0.83166245, ..., 0.61427323, 0.86546441,\n        0.80870765],\n       [0.72664455, 1.        , 0.85067939, ..., 0.90681799, 0.85595887,\n        0.85662117],\n       [0.83166245, 0.85067939, 1.        , ..., 0.75421242, 0.76655244,\n        0.73367125],\n       ...,\n       [0.61427323, 0.90681799, 0.75421242, ..., 1.        , 0.76080242,\n        0.7993527 ],\n       [0.86546441, 0.85595887, 0.76655244, ..., 0.76080242, 1.        ,\n        0.98860272],\n       [0.80870765, 0.85662117, 0.73367125, ..., 0.7993527 , 0.98860272,\n        1.        ]]) is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m svm \u001b[38;5;241m=\u001b[39m \u001b[43mSVC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(predictions, y_test))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 255\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:315\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    302\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m--> 315\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "File \u001b[1;32msklearn\\svm\\_libsvm.pyx:173\u001b[0m, in \u001b[0;36msklearn.svm._libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array([[1.        , 0.72664455, 0.83166245, ..., 0.61427323, 0.86546441,\n        0.80870765],\n       [0.72664455, 1.        , 0.85067939, ..., 0.90681799, 0.85595887,\n        0.85662117],\n       [0.83166245, 0.85067939, 1.        , ..., 0.75421242, 0.76655244,\n        0.73367125],\n       ...,\n       [0.61427323, 0.90681799, 0.75421242, ..., 1.        , 0.76080242,\n        0.7993527 ],\n       [0.86546441, 0.85595887, 0.76655244, ..., 0.76080242, 1.        ,\n        0.98860272],\n       [0.80870765, 0.85662117, 0.73367125, ..., 0.7993527 , 0.98860272,\n        1.        ]]) is not in list"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel=km).fit(X_train, y_train)\n",
    "\n",
    "predictions = svm.predict(X_train)\n",
    "print(\"acc:\", accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8fefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "def KNobjective(params):\n",
    "    nn = int(params['n_neighbors'])\n",
    "    wt = params['weights']\n",
    "    ls = int(params['leaf_size'])\n",
    "    p_ = int(params['p'])\n",
    "    clf = KNeighborsClassifier(n_neighbors=nn, weights=wt, leaf_size=ls, p=p_, n_jobs=-1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    #score = cross_val_score(clf, X_train, y_train, cv=3, scoring='accuracy').mean()\n",
    "    preds = clf.predict(x_val)\n",
    "    score = accuracy_score(y_val, preds)\n",
    "    \n",
    "    return 1/score\n",
    "\n",
    "def KNoptimize(trial):\n",
    "    params={'n_neighbors':hp.uniform('n_neighbors',2,10),\n",
    "            'weights':hp.choice('weights', ['distance', 'uniform']),\n",
    "            'leaf_size':hp.uniform('leaf_size',10,50),\n",
    "            'p':hp.choice('p',[1, 2])}\n",
    "            \n",
    "    best = fmin(fn=KNobjective,space=params,algo=tpe.suggest,trials=trial,max_evals=1000,rstate=np.random.default_rng(seed))\n",
    "    return best\n",
    "\n",
    "trial = Trials()\n",
    "KN_best = KNoptimize(trial)\n",
    "\n",
    "if KN_best[\"weights\"] == 0:\n",
    "    KN_best[\"weights\"] = 'distance'\n",
    "else:\n",
    "    KN_best[\"weights\"] = 'uniform'\n",
    "    \n",
    "if KN_best[\"p\"] == 0:\n",
    "    KN_best[\"p\"] = 1\n",
    "else:\n",
    "    KN_best[\"p\"] = 2\n",
    "    \n",
    "KN_best[\"leaf_size\"] = int(KN_best[\"leaf_size\"])\n",
    "KN_best[\"n_neighbors\"] = int(KN_best[\"n_neighbors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "KN_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f7179",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = KN_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e531fe",
   "metadata": {},
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    b_accuracy = balanced_accuracy_score(y_test,predictions)\n",
    "    accuracy = accuracy_score(y_test,predictions)\n",
    "    print('Model Performance')\n",
    "    print('Balanced accuracy = {:0.2f}%.'.format(b_accuracy))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6055bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "KN = KNeighborsClassifier().fit(x_train, y_train)\n",
    "KN_opt = KNeighborsClassifier(**params).fit(x_train, y_train)\n",
    "\n",
    "KN_predictions = KN.predict(x_val)\n",
    "KN_opt_predictions = KN_opt.predict(x_val)\n",
    "\n",
    "print('Accuracy of KNeighbors classifier: ', accuracy_score(y_val, KN_predictions))\n",
    "print('Balanced Accuracy of KNeighbors classifier: ', balanced_accuracy_score(y_val, KN_predictions))\n",
    "print('\\n')\n",
    "print('Accuracy of KNeighbors_opt classifier: ', accuracy_score(y_val, KN_opt_predictions))\n",
    "print('Balanced Accuracy of KNeighbors_opt classifier: ', balanced_accuracy_score(y_val, KN_opt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e887d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "KN.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf42c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "KN_opt.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c53c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, KN_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e7f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, KN_opt_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cdfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
