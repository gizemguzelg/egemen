{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b683f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libsss import *\n",
    "\n",
    "def optimizer(x_train, y_train, x_val, y_val):\n",
    "    X_train, y_train, X_test, y_test = x_train, y_train, x_val, y_val\n",
    "    seed=42\n",
    "    \n",
    "    c = True\n",
    "    while c:\n",
    "        model = str(input(\"Model seç: \"))\n",
    "        \n",
    "#################### LR Params\n",
    "        if model == \"LR\":\n",
    "        \n",
    "            def LRobjective(params):\n",
    "                clf = LogisticRegression(random_state = 42, **params, verbose=0, n_jobs = -1)\n",
    "                clf.fit(X_train,y_train)\n",
    "                score = cross_val_score(clf, X_train, y_train, cv=3, scoring='balanced_accuracy').mean()    \n",
    "                return 1/score\n",
    "\n",
    "            def LRoptimize(trial):\n",
    "                params={\"C\": hp.loguniform(\"C\", np.log(0.001), np.log(0.2)),\n",
    "                        'max_iter': hp.choice('max_iter', np.arange(100, 1000, dtype=int)),\n",
    "                        \"class_weight\": hp.choice(\"class_weight\", ['balanced', None]),\n",
    "                        \"solver\": hp.choice(\"solver\",['newton-cg', 'lbfgs', 'sag', 'saga'])}\n",
    "\n",
    "                best = fmin(fn = LRobjective, space = params, algo = tpe.suggest, trials = trial, max_evals = 1000,\n",
    "                            rstate = np.random.default_rng(seed))\n",
    "                return best\n",
    "\n",
    "            trial = Trials()\n",
    "            LR_best = LRoptimize(trial)\n",
    "\n",
    "            LR_best[\"max_iter\"] = np.arange(100, 1000, dtype=int)[LR_best[\"max_iter\"]]\n",
    "\n",
    "            if LR_best[\"solver\"] == 0:\n",
    "                LR_best[\"solver\"] = 'newton-cg'\n",
    "            elif LR_best[\"solver\"] == 1:\n",
    "                LR_best[\"solver\"] = 'lbfgs'\n",
    "            elif LR_best[\"solver\"] == 2:\n",
    "                LR_best[\"solver\"] = 'sag'\n",
    "            elif LR_best[\"solver\"] == 3:\n",
    "                LR_best[\"solver\"] = 'saga'\n",
    "\n",
    "            if LR_best[\"class_weight\"] == 0:\n",
    "                LR_best[\"class_weight\"] = 'balanced'\n",
    "            elif LR_best[\"class_weight\"] == 1:\n",
    "                LR_best[\"class_weight\"] = None\n",
    "\n",
    "            LR_best['n_jobs'] = -1\n",
    "            c = False\n",
    "\n",
    "            return LR_best\n",
    "\n",
    "#################### XGB Params\n",
    "        elif model == \"XGB\":\n",
    "            def XGBobjective(params):\n",
    "                clf = XGBClassifier(**params, use_label_encoder = False, verbosity = 0, n_jobs=-1)\n",
    "                clf.fit(X_train,y_train)\n",
    "                score = cross_val_score(clf, X_train, y_train, cv=3, scoring='balanced_accuracy').mean()\n",
    "                return 1/score\n",
    "\n",
    "            def XGBoptimize(trial):\n",
    "                params={\"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "                        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "                        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "                        \"max_depth\": hp.choice(\"max_depth\", np.arange(5, 30, dtype=int)),\n",
    "                        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "                        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 1000, 1, dtype=int)),\n",
    "                        \"subsample\": hp.uniform(\"subsample\", 0.3, 1)}\n",
    "\n",
    "                best = fmin(fn = XGBobjective, space = params, algo = tpe.suggest, trials = trial, max_evals = 500, \n",
    "                            rstate = np.random.default_rng(seed))\n",
    "                return best\n",
    "\n",
    "            trial = Trials()\n",
    "            XGB_best = XGBoptimize(trial)\n",
    "\n",
    "            XGB_best['use_label_encoder'] = False\n",
    "            XGB_best[\"max_depth\"] = np.arange(5, 30, dtype=int)[XGB_best[\"max_depth\"]]\n",
    "            XGB_best[\"n_estimators\"] =  np.arange(100, 1000, 1, dtype=int)[XGB_best[\"n_estimators\"]]\n",
    "            c = False\n",
    "\n",
    "            return XGB_best\n",
    "\n",
    "#################### RF Params \n",
    "        elif model == \"RF\":\n",
    "            def RFobjective(params):\n",
    "                clf = RandomForestClassifier(random_state = 42, **params, n_jobs = -1)\n",
    "                clf.fit(X_train,y_train)\n",
    "                score = cross_val_score(clf, X_train, y_train, cv=3, scoring='balanced_accuracy').mean()\n",
    "                return 1/score\n",
    "\n",
    "            def RFoptimize(trial):\n",
    "                params={\"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "                        'n_estimators': hp.choice('n_estimators', np.arange(100, 1000, dtype=int)),\n",
    "                        \"max_features\": hp.choice(\"max_features\", [\"log2\", \"sqrt\"]),\n",
    "                        'max_depth':hp.choice('max_depth', np.arange(5, 30, dtype=int)),\n",
    "                        'min_samples_leaf':hp.choice('min_samples_leaf', np.arange(1, 10, dtype=int)),\n",
    "                        'min_samples_split':hp.choice('min_samples_split', np.arange(2, 10, dtype=int))}\n",
    "\n",
    "                best = fmin(fn = RFobjective, space = params, algo = tpe.suggest, trials = trial, max_evals = 700,\n",
    "                            rstate = np.random.default_rng(seed))\n",
    "                return best\n",
    "\n",
    "            trial = Trials()\n",
    "            RF_best = RFoptimize(trial)\n",
    "\n",
    "            RF_best[\"n_estimators\"] = np.arange(100, 1000, dtype=int)[RF_best[\"n_estimators\"]]\n",
    "            RF_best[\"max_depth\"] = np.arange(5, 30, dtype=int)[RF_best[\"max_depth\"]]\n",
    "            RF_best[\"min_samples_leaf\"] =  np.arange(1, 10, dtype=int)[RF_best[\"min_samples_leaf\"]]\n",
    "            RF_best[\"min_samples_split\"] =  np.arange(2, 10, dtype=int)[RF_best[\"min_samples_split\"]]\n",
    "\n",
    "            if RF_best[\"bootstrap\"] == 0:\n",
    "                RF_best[\"bootstrap\"] = True\n",
    "            else:\n",
    "                RF_best[\"bootstrap\"] = False\n",
    "\n",
    "            if RF_best[\"max_features\"] == 0:\n",
    "                RF_best[\"max_features\"] = \"log2\"\n",
    "            else:\n",
    "                RF_best[\"max_features\"] = \"sqrt\"\n",
    "\n",
    "            RF_best['n_jobs'] = -1\n",
    "            c = False\n",
    "\n",
    "            return RF_best\n",
    "\n",
    "#################### KN Params\n",
    "        elif model == \"KN\":\n",
    "            def KNobjective(params):\n",
    "                clf = KNeighborsClassifier(**params, n_jobs=-1)\n",
    "                clf.fit(X_train,y_train)\n",
    "                score = cross_val_score(clf, X_train, y_train, cv=3, scoring='balanced_accuracy').mean()\n",
    "                return 1/score\n",
    "\n",
    "            def KNoptimize(trial):\n",
    "                params={'n_neighbors':hp.choice('n_neighbors',np.arange(2, 10, dtype=int)),\n",
    "                        'weights':hp.choice('weights', ['distance', 'uniform']),\n",
    "                        'leaf_size':hp.choice('leaf_size',np.arange(10, 50, dtype=int)),\n",
    "                        'p':hp.choice('p',[1, 2])}\n",
    "\n",
    "                best = fmin(fn = KNobjective, space = params, algo = tpe.suggest, trials = trial, max_evals = 700,\n",
    "                            rstate = np.random.default_rng(seed))\n",
    "                return best\n",
    "\n",
    "            trial = Trials()\n",
    "            KN_best = KNoptimize(trial)\n",
    "\n",
    "            KN_best[\"n_neighbors\"] = np.arange(2, 10, dtype=int)[KN_best[\"n_neighbors\"]]\n",
    "            KN_best[\"leaf_size\"] = np.arange(10, 50, dtype=int)[KN_best[\"leaf_size\"]]\n",
    "\n",
    "            if KN_best[\"weights\"] == 0:\n",
    "                KN_best[\"weights\"] = 'distance'\n",
    "            else:\n",
    "                KN_best[\"weights\"] = 'uniform'\n",
    "\n",
    "            if KN_best[\"p\"] == 0:\n",
    "                KN_best[\"p\"] = 1\n",
    "            else:\n",
    "                KN_best[\"p\"] = 2\n",
    "\n",
    "            c = False\n",
    "            return KN_best\n",
    "\n",
    "        else: \n",
    "            print(\"Geçerli model seçenekleri: LR, XGB, RF, KN\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
