{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df4e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from libsss import *\n",
    "from model_optimizer import *\n",
    "from tester import *\n",
    "\n",
    "from model_opt import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fe4786",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing insufficient data for company ...\n",
      "total number of removed data:  73\n",
      "persentage of removed data:  6.54 %\n",
      "\n",
      "\n",
      "removing insufficient data for product ...\n",
      "total number of removed data:  293\n",
      "persentage of removed data:  28.09 %\n",
      "\n",
      "\n",
      "Cleaning noise ... \n",
      "total number of removed data:  18\n",
      "persentage of removed data:  2.4 %\n",
      "\n",
      "\n",
      "Cleaning time noise ... \n",
      "total number of removed data:  56\n",
      "persentage of removed data:  7.65 %\n",
      "\n",
      " values: \n",
      "time\n",
      "1     395\n",
      "2     106\n",
      "3      80\n",
      "4      46\n",
      "5      18\n",
      "6      14\n",
      "7       5\n",
      "9       3\n",
      "10      2\n",
      "11      2\n",
      "12      1\n",
      "14      2\n",
      "34      1\n",
      "44      1\n",
      "dtype: int64\n",
      "\\h:  395\n",
      "\n",
      "\n",
      "Mapping time\n",
      "[1]  -------->  0\n",
      "[2]  -------->  1\n",
      "[3]  -------->  2\n",
      "[4, 5]  -------->  3\n",
      "[6, 7, 9]  -------->  4\n",
      "insuff_time:  [10, 11, 12, 14, 34, 44]\n",
      "classes:  [0, 1, 2, 3, 4]\n",
      "\n",
      "\n",
      "one hot encoding  week day ...\n",
      "week day encoded.\n",
      "one hot encoding  company ...\n",
      "company encoded.\n",
      "one hot encoding  product ...\n",
      "product encoded.\n",
      "one hot encoding  order month ...\n",
      "order month encoded.\n",
      "\n",
      "\n",
      "number of features dropped from  61  to  36\n",
      "\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "def accuracy_calculate(actual_values, predicted_values):\n",
    "    comparison = abs(np.round(predicted_values) - actual_values)\n",
    "    accuracy = 1- ((len(comparison[comparison>=(0+1)])) / len(actual_values))\n",
    "    return accuracy\n",
    "\n",
    "def mean_iou_calculator(actual_values, predicted_values, time):\n",
    "    confusion_array = confusion_matrix(actual_values, predicted_values)\n",
    "    individual_ious = []\n",
    "    for i in range(len(confusion_array)):\n",
    "        individual_iou = confusion_array[i][i] / (sum(confusion_array[i]))\n",
    "        individual_ious.append(individual_iou)\n",
    "    mean_iou = sum(individual_ious)/len(individual_ious)\n",
    "    results = pd.DataFrame()  \n",
    "    featue_y_values = sorted(data[time].unique())\n",
    "    for i in range(len(individual_ious)):\n",
    "        results.insert(0, 'iou_(' + str(featue_y_values[i])  +')', [individual_ious[i]], True)\n",
    "    results = results[results.columns[::-1]]\n",
    "    results.insert(0, 'mean_iou', mean_iou, True)\n",
    "    return results\n",
    "\n",
    "\n",
    "########################################################################################### READ CSV \n",
    "def read(csv):\n",
    "    data = pd.read_csv(csv, sep=';', encoding = \"utf8\")\n",
    "    #Rearannge the dataframe as the old one\n",
    "    data.columns = ['product', 'amount', 'company', 'town', 'type', 'order date', 'delivery date', 'time']\n",
    "\n",
    "    data2 = data.drop('delivery date', axis = 1)\n",
    "    data2[\"order day\"] = ''\n",
    "    data2[\"order month\"] = ''\n",
    "    data2[\"week day\"] = ''\n",
    "\n",
    "    # Remove 'order date' and add 'order day', 'order month' and 'week day' features\n",
    "    for i in range(len(data2)):\n",
    "      data2.at[i, 'order day'] = data2['order date'][i].split()[0]\n",
    "      data2.at[i, 'order month'] = data2['order date'][i].split()[1]\n",
    "      data2.at[i, 'week day'] = data2['order date'][i].split()[-1]\n",
    "    data2 = data2.drop('order date', axis = 1)\n",
    "    data2['week day'] = data2['week day'].str.replace('Pazartesi','pts')\n",
    "    data2['week day'] = data2['week day'].str.replace('Salı','sal')\n",
    "    data2['week day'] = data2['week day'].str.replace('Çarşamba','çrş')\n",
    "    data2['week day'] = data2['week day'].str.replace('Perşembe','prş')\n",
    "    data2['week day'] = data2['week day'].str.replace('Cumartesi','cts')\n",
    "    data2['week day'] = data2['week day'].str.replace('Cuma','cum')\n",
    "    data2['week day'] = data2['week day'].str.replace('Pazar','paz')\n",
    "\n",
    "    # data2 = data2[data2[\"week day\"].str.contains(\"Pazar\") == False]\n",
    "\n",
    "    # Rearranging Dataframe\n",
    "    data2 = data2[['product', 'company', 'amount', 'town', 'type', 'order day', 'week day', 'order month', 'time']]\n",
    "    data2['town'] = data2['town'].str.lower()\n",
    "    data2['town'] = data2['town'].str.replace('i̇','i')\n",
    "    data2['town'] = data2['town'].str.replace('.','missing')\n",
    "    data2['town'] = data2['town'].str.replace(' tekirdağ','tekirdağ')\n",
    "    data2['town'] = data2['town'].str.replace('küçükçekmece','istanbul')\n",
    "    data2['town'] = data2['town'].str.replace('çorlu','tekirdağ')\n",
    "    data2['town'] = data2['town'].str.replace('bandirma','balıkesir')\n",
    "\n",
    "    #data2 = data2.drop('town',axis = 1).reset_index(drop=True)\n",
    "    data2 = data2.drop('order day',axis = 1).reset_index(drop=True)\n",
    "    #data2 = data2.drop('order month',axis = 1).reset_index(drop=True)\n",
    "    data2 = data2.fillna(\"missing\")\n",
    "    #data2 = data2[data2[\"town\"].str.contains(\"missing\") == False]\n",
    "\n",
    "    data_clean = data2.copy()\n",
    "    drop_df = data2.copy()\n",
    "    drop_index_list = []\n",
    "    return data2, data_clean ,drop_df, drop_index_list\n",
    "\n",
    "data2, data_clean ,drop_df, drop_index_list = read('gulle.csv')\n",
    "\n",
    "\n",
    "########################################################################################### REMOVE INSUFFICIENT DATA\n",
    "def remove_insuff(df, ft):\n",
    "    print(\"removing insufficient data for\", ft, \"...\")\n",
    "    fst_len = len(df)\n",
    "    x = df[ft].value_counts() < 10\n",
    "    df2 = x.to_frame().reset_index()\n",
    "    df2.columns = [ft, 'booly']\n",
    "    df2.drop(df2[df2.booly == False].index, inplace=True)\n",
    "    drop_list = df2[ft].tolist()\n",
    "    drop_indices=[]\n",
    "\n",
    "    if len(drop_list) != 0:\n",
    "        for i in df.index:\n",
    "            for j in range(len(drop_list)):\n",
    "                if (drop_list[j] == df.at[i, ft]):\n",
    "                    drop_indices = drop_indices + [i]\n",
    "        df.drop(drop_indices, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        drop_indices = []\n",
    "                        \n",
    "    lst_len = len(df)\n",
    "    rem = fst_len - lst_len      # number of removed data\n",
    "    per = (rem / fst_len) * 100  # percentage of removed data\n",
    "\n",
    "    print(\"total number of removed data: \", rem)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    return df, drop_indices\n",
    "\n",
    "data2, drop_indices = remove_insuff(data2, \"company\")\n",
    "drop_index_list += drop_indices\n",
    "print(\"\\n\")\n",
    "\n",
    "data2, drop_indices = remove_insuff(data2, \"product\")\n",
    "drop_index_list += drop_indices\n",
    "\n",
    "\n",
    "########################################################################################### CLEAN NOISE\n",
    "print(\"\\n\")\n",
    "def clean_noise(df): \n",
    "    in_len = len(df)\n",
    "    zs = 0.89\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "    \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                max_min_a = df_max_scaled2[\"amount\"].max() - df_max_scaled2[\"amount\"].min()\n",
    "                \n",
    "                if (max_min_a != 0) and (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zs\n",
    "\n",
    "                    df_max_scaled2[\"amount\"] = (df_max_scaled2[\"amount\"] - df_max_scaled2[\"amount\"].min()) / max_min_a\n",
    "                    amo_sc = df_max_scaled2[\"amount\"]\n",
    "                    df_zscore_a = (amo_sc - amo_sc.mean())/amo_sc.std()\n",
    "                    dfz_a = abs(df_zscore_a) > zs\n",
    "\n",
    "                    df1 = dfz_t[\"time\"] & dfz_a \n",
    "                    df2 = (df_zscore_t[\"time\"] * df_zscore_a) < 0 \n",
    "                    dfz = df1 & df2 \n",
    "\n",
    "                    index_drop_list = index_drop_list + [*filter(dfz.get, dfz.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    #print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list\n",
    "\n",
    "def clean_noise_tt(df): \n",
    "    in_len = len(df)\n",
    "    zst = 1.5\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "        \n",
    "    print(\"Cleaning time noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                \n",
    "                if (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zst\n",
    "                    \n",
    "                    index_drop_list += dfz_t[dfz_t[\"time\"].eq(True)].index.tolist()\n",
    "                    \n",
    "                    #index_drop_list = index_drop_list + [*filter(dfz_t.get, dfz_t.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    #print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "data2, train_drop_list = clean_noise(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list\n",
    "print(\"\\n\")\n",
    "\n",
    "data2, train_drop_list = clean_noise_tt(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list\n",
    "\n",
    "print(\"\\n\", \"values: \")\n",
    "print(data2.groupby('time').size())\n",
    "\n",
    "\n",
    "########################################################################################### GROUP TIME\n",
    "def group_time(df):\n",
    "    min_samp = len(df) * 0.07\n",
    "    t = 1\n",
    "    gs = 0\n",
    "    group = []\n",
    "    sub_group = [t]\n",
    "    #gap = df[\"time\"].std()\n",
    "\n",
    "    while t <= df[\"time\"].max():\n",
    "        \n",
    "        \n",
    "        if sum(df[\"time\"] == t) > 0:\n",
    "        \n",
    "            gs += sum(df[\"time\"] == t)\n",
    "\n",
    "            if (len(sub_group) > 0) and ((t - min(sub_group)) <= math.ceil(math.sqrt(min(sub_group)))):\n",
    "                sub_group += [t]\n",
    "\n",
    "            else:\n",
    "                if (len(sub_group) != 0):\n",
    "                    gs = 0\n",
    "                    group += [sub_group]\n",
    "                sub_group = [t]\n",
    "\n",
    "            if (gs >= min_samp) or ((t - min(sub_group)) > math.ceil(math.sqrt(min(sub_group)))):\n",
    "                gs = 0\n",
    "                group += [sub_group]\n",
    "                sub_group = []\n",
    "                \n",
    "        if t == df[\"time\"].max() :\n",
    "            if len(sub_group) != 0:\n",
    "                group += [sub_group]\n",
    "            group[0].remove(1)\n",
    "            \n",
    "        t += 1\n",
    "        \n",
    "    return group\n",
    "\n",
    "groups = group_time(data2)\n",
    "\n",
    "\n",
    "########################################################################################### REMOVE INSUFFICIENT GROUPS\n",
    "def remove_insuff_groups(df, groups):\n",
    "    \n",
    "    h = ()\n",
    "    for i in range(len(groups)):\n",
    "        a = 0\n",
    "        for j in range(len(groups[i])):\n",
    "            a += sum(df[\"time\"] == groups[i][j])\n",
    "        h += (a,)\n",
    "    h = max(h)\n",
    "    print(\"\\h: \", h)\n",
    "    \n",
    "    \n",
    "    c = True     # herhangi bir sub_groupta bulunan zamanlar dataframede 10 kereden az bulunuyorsa bu sub_groupu sil\n",
    "    insuff_time = []\n",
    "    while c:\n",
    "        for i in range(len(groups)):\n",
    "            s = 0\n",
    "            for j in range(len(groups[i])):\n",
    "                s += sum(df[\"time\"] == groups[i][j])\n",
    "            if s < (h * 0.05):           #----------------------------->>>>> fix\n",
    "                insuff_time += groups[i]\n",
    "        c = False\n",
    "\n",
    "    return insuff_time\n",
    "\n",
    "insuff_time = remove_insuff_groups(data2, groups)\n",
    "\n",
    "\n",
    "########################################################################################### MAP TIME\n",
    "def map_time(df, groups, insuff_time):\n",
    "    print(\"Mapping time\")\n",
    "    index_drop_list = []\n",
    "    for i in df.index:\n",
    "        for t in insuff_time:\n",
    "            if df.at[i, \"time\"] == t:\n",
    "                index_drop_list += [i]\n",
    "                \n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "\n",
    "    time = -1\n",
    "    for sub_group in groups:\n",
    "        \n",
    "        time += 1\n",
    "        for t in sub_group:\n",
    "            for i in df.index:\n",
    "                if df.at[i, \"time\"] == t:\n",
    "                    df.at[i, \"time\"] = time\n",
    "                    \n",
    "        if sub_group[0] not in insuff_time:\n",
    "            print( sub_group, \" --------> \", time)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\"\\n\")\n",
    "data2 = map_time(data2, groups, insuff_time)\n",
    "print(\"insuff_time: \", insuff_time)\n",
    "print(\"classes: \", sorted(data2[\"time\"].unique()))\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################### ONE HOT ENCODING\n",
    "def one_hot(df, ft):      ### ft = \"company\", \"product\", \"week day\" etc.\n",
    "    print(\"one hot encoding \", ft, \"...\")\n",
    "    dum = pd.get_dummies(df[ft])\n",
    "    df = df.drop(ft, axis = 1)\n",
    "    df = df.join(dum)\n",
    "    print(ft, \"encoded.\")\n",
    "    return df\n",
    "\n",
    "print(\"\\n\")\n",
    "data2  = one_hot(data2, \"week day\")\n",
    "data2  = one_hot(data2, \"company\")\n",
    "data2  = one_hot(data2, \"product\")\n",
    "data2  = one_hot(data2, \"order month\")\n",
    "\n",
    "\n",
    "########################################################################################### DROP_DF & DATA_CLEAN\n",
    "drop_index_list = sorted(list(set(drop_index_list)))\n",
    "drop_df = drop_df.loc[drop_index_list]\n",
    "\n",
    "data_clean.drop(drop_df.index.to_list(), axis=0, inplace=True)\n",
    "\n",
    "\n",
    "########################################################################################### DROP TOWN COLUMN\n",
    "data2 = data2.drop('town',axis = 1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "########################################################################################### TRAIN-TEST SPLIT\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data2.copy()\n",
    "Y = data2.copy()\n",
    "X.drop(\"time\", axis=1, inplace=True)\n",
    "Y = Y[\"time\"]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "########################################################################################### NORMALIZE AMOUNT\n",
    "xt_min = x_train[\"amount\"].min()\n",
    "xt_max = x_train[\"amount\"].max()\n",
    "\n",
    "x_train[\"amount\"] = (x_train[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "x_val[\"amount\"] = (x_val[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "\n",
    "\n",
    "########################################################################################### DF_EMPTY\n",
    "df_empty = x_train[0:0]\n",
    "\n",
    "\n",
    "########################################################################################### PCA\n",
    "def do_pca(x_train, x_val):\n",
    "    from sklearn.decomposition import PCA\n",
    "    xl = len(x_train.columns)\n",
    "    pca = PCA(.95)\n",
    "    pca.fit(x_train)\n",
    "    print(\"number of features dropped from \", xl, \" to \", pca.n_components_) \n",
    "    #print(\"variance ratio: \", pca.explained_variance_ratio_) \n",
    "\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_val = pca.transform(x_val)\n",
    "    return pca, x_train, x_val\n",
    "\n",
    "print(\"\\n\")\n",
    "pca, x_train, x_val = do_pca(x_train, x_val)\n",
    "\n",
    "print(\"\\ndone...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e0d3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizerr(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b273c9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing LogisticRegression Parameters...\n",
      "100%|████████████████████████████████████████████| 1000/1000 [01:38<00:00, 10.12trial/s, best loss: 3.6416506697112685]\n",
      "\n",
      "Optimizing XGBClassifier Parameters...\n",
      "  8%|███▌                                          | 77/1000 [06:33<1:43:38,  6.74s/trial, best loss: 2.73733708212279]"
     ]
    }
   ],
   "source": [
    "LR_best, XGB_best, RF_best, KN_best, MLP_best = opt_params(x_train, y_train, x_val, y_val, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09600a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LR_opt, XGB_opt, RF_opt, KN_opt, MLP_opt = get_results(LR_best, XGB_best, RF_best, KN_best, MLP_best, x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80254f",
   "metadata": {},
   "source": [
    "# Take input and create df_inp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163cdfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prod_in_data(df):\n",
    "    prods = df[\"product\"].unique()\n",
    "    prd = str(input(\"Product seç: \"))\n",
    "    if prd not in prods: \n",
    "        return False, prd\n",
    "    else:\n",
    "        return True, prd\n",
    "    \n",
    "def is_prod_in_data_drop(df, prd):\n",
    "    prods = df[\"product\"].unique()\n",
    "    if prd not in prods: \n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def take_input(df, prd):\n",
    "    amo = input(\"Amount: \") \n",
    "    y = False\n",
    "    while y == False:\n",
    "        if (amo.isnumeric() == False):\n",
    "            print(\"Pozitif tam sayı değer giriniz\")\n",
    "            amo = input(\"Amount: \")\n",
    "\n",
    "        else:\n",
    "            d = max(df[df[\"product\"] == prd][\"amount\"].to_list()) * 3\n",
    "            if int(amo) > d:\n",
    "                print(\"Amount yüksek abi emin misin bak !?\")\n",
    "                y_n = input(\"y / n ?\")\n",
    "                if y_n == \"y\":\n",
    "                    y = True\n",
    "                elif y_n == \"n\":\n",
    "                    amo = input(\"Amount: \")                   \n",
    "            else:\n",
    "                y = True\n",
    "    amo = int(amo)\n",
    "\n",
    "    w_days = ['pts', 'sal', 'çrş', 'prş', 'cum', 'cts', 'paz']\n",
    "    wd = str(input(\"Week day: \"))\n",
    "    z = False\n",
    "    while z == False:\n",
    "        if wd not in w_days:\n",
    "            print(\"Geçerli gün giriniz...\")\n",
    "            print(\"Geçerli günler: \", w_days)\n",
    "            wd = str(input(\"Week day: \"))\n",
    "        else:\n",
    "            z = True\n",
    "            \n",
    "            \n",
    "    months = ['Ocak', 'Şubat', 'Mart', 'Nisan', 'Mayıs', 'Haziran', 'Temmuz', 'Ağustos', 'Eylül', 'Ekim', 'Kasım', 'Aralık']\n",
    "    month = str(input(\"Ay: \"))\n",
    "    z = False\n",
    "    while z == False:\n",
    "        if month not in months:\n",
    "            print(\"Geçerli ay giriniz...\")\n",
    "            print(\"Geçerli aylar: \", months)\n",
    "            month = str(input(\"Ay: \"))\n",
    "        else:\n",
    "            z = True\n",
    "\n",
    "    typ = df[df[\"product\"] == prd][\"type\"].unique()[0] \n",
    "    comps = df[df[\"product\"] == prd][\"company\"].unique()\n",
    "    tws = df[df[\"product\"] == prd][\"town\"].unique()\n",
    "\n",
    "    df_inp = pd.DataFrame(columns = ['product', 'company', 'amount', 'town', 'type', 'week day', 'order month'])\n",
    "    \n",
    "    for comp in comps:\n",
    "        tw = df[(df[\"product\"] == prd) & (df[\"company\"] == comp)][\"town\"].unique()[0]\n",
    "\n",
    "        df_inp = df_inp.append({'product' : prd, 'company' : comp, 'order month': month, 'amount' : amo, \n",
    "                                'town' : tw,'type' : typ, 'week day' : wd}, ignore_index=True)\n",
    "\n",
    "    return df_inp, comps\n",
    "\n",
    "def run():\n",
    "    cond1 = False\n",
    "    while cond1 == False:\n",
    "        tf, prd = is_prod_in_data(data_clean)\n",
    "        \n",
    "        if tf:\n",
    "            df_inp, comps = take_input(data_clean, prd)\n",
    "            c = False\n",
    "            cond1 = True\n",
    "        else:\n",
    "            if (is_prod_in_data_drop(drop_df, prd) == True):\n",
    "                print(\"Güvenilir sonuç için product'a ait en az 5 giriş bulunmalıdır.\")\n",
    "                print(\"\\n\",\"Daha önce bu product alımları: \")\n",
    "                a = drop_df[drop_df[\"product\"] == prd]\n",
    "                a = a.index.to_list()\n",
    "                a = data.loc[a]\n",
    "                df_inp = display(a[['company', 'amount', 'town', 'order date', 'delivery date', 'time']].style.hide(axis='index'))\n",
    "                print (\"Ortalama miktar = \", a[\"amount\"].mean(), \"Ortalama süre = \", a[\"time\"].mean())\n",
    "                comps = a[\"company\"].unique()\n",
    "                return df_inp, cond1, comps\n",
    "    \n",
    "            else:\n",
    "                df_inp = print(\"Product bulunamadı.\")\n",
    "\n",
    "    if cond1:\n",
    "        df_inp  = one_hot(df_inp, \"week day\")\n",
    "        df_inp  = one_hot(df_inp, \"product\")\n",
    "        df_inp  = one_hot(df_inp, \"company\")\n",
    "        df_inp  = one_hot(df_inp, \"order month\")\n",
    "\n",
    "        df_inp = df_inp.drop('town',axis = 1).reset_index(drop=True)\n",
    "\n",
    "        df_inp[\"amount\"] = (df_inp[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "\n",
    "        df_inp = df_empty.append(df_inp)   # for the next version of pandas use the next line of code instead of this one. \n",
    "        #df_inp = pd.concat([df_inp, df_empty])   ------->>> for the future version of pandas\n",
    "        \n",
    "        df_inp = df_inp.fillna(0)\n",
    "        df_inp = pca.transform(df_inp)\n",
    "        \n",
    "    return df_inp, cond1, comps\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edda0cc",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd90a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product seç: K730\n",
      "Amount: 444\n",
      "Week day: sal\n",
      "Ay: OCak\n",
      "Geçerli ay giriniz...\n",
      "Geçerli aylar:  ['Ocak', 'Şubat', 'Mart', 'Nisan', 'Mayıs', 'Haziran', 'Temmuz', 'Ağustos', 'Eylül', 'Ekim', 'Kasım', 'Aralık']\n",
      "Ay: Ocak\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>company</th>\n",
       "      <th>amount</th>\n",
       "      <th>town</th>\n",
       "      <th>type</th>\n",
       "      <th>week day</th>\n",
       "      <th>order month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K730</td>\n",
       "      <td>T-060</td>\n",
       "      <td>444</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>Ocak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K730</td>\n",
       "      <td>T-0142</td>\n",
       "      <td>444</td>\n",
       "      <td>missing</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>Ocak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product company amount      town type week day order month\n",
       "0    K730   T-060    444  tekirdağ    1      sal        Ocak\n",
       "1    K730  T-0142    444   missing    1      sal        Ocak"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoding  week day ...\n",
      "week day encoded.\n",
      "one hot encoding  product ...\n",
      "product encoded.\n",
      "one hot encoding  company ...\n",
      "company encoded.\n",
      "one hot encoding  order month ...\n",
      "order month encoded.\n",
      "LR:  [1 0] \n",
      "RF: [0 0] \n",
      "KN: [0 1] \n",
      "XGB: [0 1]\n",
      "for company  T-060  predicted time =  0.0\n",
      "for company  T-0142  predicted time =  0.0\n"
     ]
    }
   ],
   "source": [
    "df_inp, cond1, comps = run()\n",
    "\n",
    "if cond1:\n",
    "    LR = LR_opt.predict(df_inp)\n",
    "    RF = RF_opt.predict(df_inp)\n",
    "    KN = KN_opt.predict(df_inp)\n",
    "    XGB = XGB_opt.predict(df_inp)\n",
    "   \n",
    "    print(\"LR: \", LR, \"\\nRF:\", RF, \"\\nKN:\", KN, \"\\nXGB:\", XGB)\n",
    "    \n",
    "    res = np.array([])\n",
    "    output = np.array([])\n",
    "    for i in range(len(LR)):\n",
    "        res = np.append(res, [LR[i], RF[i], KN[i], XGB[i]])\n",
    "        m = mode(res)[0][0]\n",
    "        output = np.append(output, m)\n",
    "        res = np.array([])\n",
    "    \n",
    "        print(\"for company \", comps[i], \" predicted time = \", output[i])\n",
    "        \n",
    "del(df_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af11e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e69f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
