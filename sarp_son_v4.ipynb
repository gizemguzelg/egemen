{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = 0.53\n",
    "\n",
    "sehirler=[\"Adana\", \"Adıyaman\", \"Afyon\", \"Ağrı\", \"Amasya\", \"Ankara\", \"Antalya\", \"Artvin\", \"Aydın\", \"Balıkesir\", \n",
    "          \"Bilecik\", \"Bingöl\", \"Bitlis\", \"Bolu\", \"Burdur\", \"Bursa\", \"Çanakkale\", \"Çankırı\", \"Çorum\", \"Denizli\", \n",
    "          \"Diyarbakır\", \"Edirne\", \"Elazığ\", \"Erzincan\", \"Erzurum\", \"Eskişehir\", \"Gaziantep\", \"Giresun\", \"Gümüşhane\", \n",
    "          \"Hakkari\", \"Hatay\", \"Isparta\", \"Mersin\", \"İstanbul\", \"İzmir\", \"Kars\", \"Kastamonu\", \"Kayseri\", \"Kırklareli\", \n",
    "          \"Kırşehir\", \"Kocaeli\", \"Konya\", \"Kütahya\", \"Malatya\", \"Manisa\", \"Kahramanmaraş\", \"Mardin\", \"Muğla\", \"Muş\", \n",
    "          \"Nevşehir\", \"Niğde\", \"Ordu\", \"Rize\", \"Sakarya\", \"Samsun\", \"Siirt\", \"Sinop\", \"Sivas\", \"Tekirdağ\", \"Tokat\", \n",
    "          \"Trabzon\", \"Tunceli\", \"Şanlıurfa\", \"Uşak\", \"Van\", \"Yozgat\", \"Zonguldak\", \"Aksaray\", \"Bayburt\", \"Karaman\", \n",
    "          \"Kırıkkale\", \"Batman\", \"Şırnak\", \"Bartın\", \"Ardahan\", \"Iğdır\", \"Yalova\", \"Karabük\", \"Kilis\", \"Osmaniye\", \"Düzce\"]\n",
    "\n",
    "for i in range(len(sehirler)):\n",
    "    sehirler[i] = sehirler[i].lower()\n",
    "    sehirler[i] = sehirler[i].replace('i̇','i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy_calculate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calculate(actual_values, predicted_values):\n",
    "\n",
    "  comparison = abs(np.round(predicted_values) - actual_values)\n",
    "  accuracy = 1- ((len(comparison[comparison>=(0+1)])) / len(actual_values))\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean_iou_calculator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou_calculator(actual_values, predicted_values, time):\n",
    "  confusion_array = confusion_matrix(actual_values, predicted_values)\n",
    "  individual_ious = []\n",
    "  for i in range(len(confusion_array)):\n",
    "    individual_iou = confusion_array[i][i] / (sum(confusion_array[i]))\n",
    "    individual_ious.append(individual_iou)\n",
    "  mean_iou = sum(individual_ious)/len(individual_ious)\n",
    "  results = pd.DataFrame()  \n",
    "  featue_y_values = sorted(data[time].unique())\n",
    "  for i in range(len(individual_ious)):\n",
    "    results.insert(0, 'iou_(' + str(featue_y_values[i])  +')', [individual_ious[i]], True)\n",
    "  results = results[results.columns[::-1]]\n",
    "  results.insert(0, 'mean_iou', mean_iou, True)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\egeme\\AppData\\Local\\Temp\\ipykernel_19104\\3020842270.py:30: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  data2['town'] = data2['town'].str.replace('.','missing')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>company</th>\n",
       "      <th>amount</th>\n",
       "      <th>town</th>\n",
       "      <th>type</th>\n",
       "      <th>week day</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.02.02.0002</td>\n",
       "      <td>S5499</td>\n",
       "      <td>7340</td>\n",
       "      <td>balıkesir</td>\n",
       "      <td>1</td>\n",
       "      <td>cum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.02.01.0009</td>\n",
       "      <td>S0217</td>\n",
       "      <td>1200</td>\n",
       "      <td>bursa</td>\n",
       "      <td>1</td>\n",
       "      <td>çrş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.02.01.0009</td>\n",
       "      <td>S0217</td>\n",
       "      <td>480</td>\n",
       "      <td>bursa</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.02.01.0009</td>\n",
       "      <td>S0217</td>\n",
       "      <td>600</td>\n",
       "      <td>bursa</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.02.01.0009</td>\n",
       "      <td>S0217</td>\n",
       "      <td>480</td>\n",
       "      <td>bursa</td>\n",
       "      <td>1</td>\n",
       "      <td>çrş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7311</th>\n",
       "      <td>30.02.08.0089</td>\n",
       "      <td>S8822</td>\n",
       "      <td>1000</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>çrş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td>30.02.08.0089</td>\n",
       "      <td>S8822</td>\n",
       "      <td>1000</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7313</th>\n",
       "      <td>30.02.08.0089</td>\n",
       "      <td>S8822</td>\n",
       "      <td>2000</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>çrş</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7314</th>\n",
       "      <td>30.02.08.0089</td>\n",
       "      <td>S8822</td>\n",
       "      <td>2000</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>sal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7315</th>\n",
       "      <td>30.02.08.0089</td>\n",
       "      <td>S8822</td>\n",
       "      <td>2000</td>\n",
       "      <td>tekirdağ</td>\n",
       "      <td>1</td>\n",
       "      <td>pts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7316 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            product company  amount       town  type week day  time\n",
       "0     30.02.02.0002   S5499    7340  balıkesir     1      cum     3\n",
       "1     30.02.01.0009   S0217    1200      bursa     1      çrş     1\n",
       "2     30.02.01.0009   S0217     480      bursa     1      pts     2\n",
       "3     30.02.01.0009   S0217     600      bursa     1      pts     2\n",
       "4     30.02.01.0009   S0217     480      bursa     1      çrş     1\n",
       "...             ...     ...     ...        ...   ...      ...   ...\n",
       "7311  30.02.08.0089   S8822    1000   tekirdağ     1      çrş     1\n",
       "7312  30.02.08.0089   S8822    1000   tekirdağ     1      pts     1\n",
       "7313  30.02.08.0089   S8822    2000   tekirdağ     1      çrş     1\n",
       "7314  30.02.08.0089   S8822    2000   tekirdağ     1      sal     2\n",
       "7315  30.02.08.0089   S8822    2000   tekirdağ     1      pts     1\n",
       "\n",
       "[7316 rows x 7 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Book10.csv', sep=';', encoding = \"utf8\")\n",
    "data.columns = ['product', 'amount', 'company', 'town', 'type', 'order date', 'delivery date', 'time'] #Rearannge the dataframe as the old one\n",
    "\n",
    "data2 = data.drop('delivery date', axis = 1)\n",
    "data2[\"order day\"] = ''\n",
    "data2[\"order month\"] = ''\n",
    "data2[\"week day\"] = ''\n",
    "\n",
    "\n",
    "# Remove 'order date' and add 'order day', 'order month' and 'week day' features\n",
    "for i in range(len(data2)):\n",
    "  data2.at[i, 'order day'] = data2['order date'][i].split()[0]\n",
    "  data2.at[i, 'order month'] = data2['order date'][i].split()[1]\n",
    "  data2.at[i, 'week day'] = data2['order date'][i].split()[-1]\n",
    "data2 = data2.drop('order date', axis = 1)\n",
    "data2['week day'] = data2['week day'].str.replace('Pazartesi','pts')\n",
    "data2['week day'] = data2['week day'].str.replace('Salı','sal')\n",
    "data2['week day'] = data2['week day'].str.replace('Çarşamba','çrş')\n",
    "data2['week day'] = data2['week day'].str.replace('Perşembe','prş')\n",
    "data2['week day'] = data2['week day'].str.replace('Cumartesi','cts')\n",
    "data2['week day'] = data2['week day'].str.replace('Cuma','cum')\n",
    "data2['week day'] = data2['week day'].str.replace('Pazar','paz')\n",
    "\n",
    "# data2 = data2[data2[\"week day\"].str.contains(\"Pazar\") == False]\n",
    "\n",
    "# Rearranging Dataframe\n",
    "data2 = data2[['product', 'company', 'amount', 'town', 'type', 'order day', 'week day', 'order month', 'time']]\n",
    "data2['town'] = data2['town'].str.lower()\n",
    "data2['town'] = data2['town'].str.replace('i̇','i')\n",
    "data2['town'] = data2['town'].str.replace('.','missing')\n",
    "data2['town'] = data2['town'].str.replace(' tekirdağ','tekirdağ')\n",
    "data2['town'] = data2['town'].str.replace('küçükçekmece','istanbul')\n",
    "data2['town'] = data2['town'].str.replace('çorlu','tekirdağ')\n",
    "data2['town'] = data2['town'].str.replace('bandirma','balıkesir')\n",
    "\n",
    "#data2 = data2.drop('town',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.drop('order day',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.drop('order month',axis = 1).reset_index(drop=True)\n",
    "data2 = data2.fillna(\"missing\")\n",
    "#data2 = data2[data2[\"town\"].str.contains(\"missing\") == False]\n",
    "\n",
    "data_clean = data2.copy()\n",
    "drop_df = data2.copy()\n",
    "drop_index_list = []\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>company</th>\n",
       "      <th>amount</th>\n",
       "      <th>town</th>\n",
       "      <th>type</th>\n",
       "      <th>week day</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [product, company, amount, town, type, week day, time]\n",
       "Index: []"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[data2[\"product\"]==\"K1046\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of abroad companies and products that are supplied from abroad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abroad company list:  ['S0826' 'S3008' 'S3027' 'S5487' 'S5489']\n",
      "abroad product list:  ['30.01.01.0183' '30.01.01.0047' '30.01.01.0121' '30.01.01.0122'\n",
      " '30.01.01.0123' '30.01.01.0124' '30.01.01.0125' '30.01.01.0126'\n",
      " '30.01.01.0127' '30.01.01.0128' '30.02.11.0017' '30.02.11.0018'\n",
      " '30.02.11.0020' '30.02.11.0028' '30.01.01.0268' '30.01.01.0269'\n",
      " '30.01.01.0270' '30.01.01.0271' '30.01.01.0272' '30.01.01.0273'\n",
      " '30.01.01.0274' '30.01.01.0060' '30.01.01.0063' '30.01.01.0065'\n",
      " '30.01.01.0071' '30.01.01.0072' '30.01.01.0129' '30.01.01.0191'\n",
      " '30.01.01.0193' '30.01.01.0255' '30.01.01.0263' '30.01.01.0265'\n",
      " '30.01.01.0266' '30.01.01.0267']\n"
     ]
    }
   ],
   "source": [
    "abr = [item for item in data2[\"town\"].unique() if item not in sehirler]\n",
    "abr_str = \"\"\n",
    "\n",
    "for i in range (len(abr)):\n",
    "    abr_str = abr_str + \"|\" + abr[i]\n",
    "abr_str = abr_str[1:]\n",
    "\n",
    "if len(abr) != 0:\n",
    "    data3 = data2[data2[\"town\"].astype('str').str.contains(abr_str) == True]\n",
    "    abr_comp_list = data3[\"company\"].unique()\n",
    "    abr_prod_list = data3[\"product\"].unique()\n",
    "    print(\"abroad company list: \",abr_comp_list)\n",
    "    print(\"abroad product list: \",abr_prod_list)\n",
    "else:\n",
    "    print(\"no abroad company\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove insufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_insuff(df, ft):\n",
    "    print(\"removing insufficient data for \", ft, \"...\")\n",
    "    fst_len = len(df)\n",
    "    x = df[ft].value_counts() < 5 \n",
    "    df2 = x.to_frame().reset_index()\n",
    "    df2.columns = [ft, 'booly']\n",
    "    df2.drop(df2[df2.booly == False].index, inplace=True)\n",
    "    drop_list = df2[ft].tolist()\n",
    "    drop_indices=[]\n",
    "\n",
    "    if len(drop_list) != 0:\n",
    "        for i in df.index:\n",
    "            for j in range(len(drop_list)):\n",
    "                if (drop_list[j] == df.at[i, ft]):\n",
    "                    drop_indices = drop_indices + [i]\n",
    "        df.drop(drop_indices, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        drop_indices = []\n",
    "                        \n",
    "    lst_len = len(df)\n",
    "    rem = fst_len - lst_len      # number of removed data\n",
    "    per = (rem / fst_len) * 100  # percentage of removed data\n",
    "\n",
    "    print(\"total number of removed data: \", rem)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    return df, drop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing insufficient data for  company ...\n",
      "total number of removed data:  38\n",
      "persentage of removed data:  0.52 %\n",
      "removing insufficient data for  product ...\n",
      "total number of removed data:  526\n",
      "persentage of removed data:  7.23 %\n"
     ]
    }
   ],
   "source": [
    "data2, drop_indices = remove_insuff(data2, \"company\")\n",
    "drop_index_list = drop_index_list + drop_indices\n",
    "\n",
    "data2, drop_indices = remove_insuff(data2, \"product\")\n",
    "drop_index_list = drop_index_list + drop_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_noise(df): \n",
    "    in_len = len(df)\n",
    "    zs = 0.89\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "    \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                max_min_a = df_max_scaled2[\"amount\"].max() - df_max_scaled2[\"amount\"].min()\n",
    "                \n",
    "                if (max_min_a != 0) and (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zs\n",
    "\n",
    "                    df_max_scaled2[\"amount\"] = (df_max_scaled2[\"amount\"] - df_max_scaled2[\"amount\"].min()) / max_min_a\n",
    "                    amo_sc = df_max_scaled2[\"amount\"]\n",
    "                    df_zscore_a = (amo_sc - amo_sc.mean())/amo_sc.std()\n",
    "                    dfz_a = abs(df_zscore_a) > zs\n",
    "\n",
    "                    df1 = dfz_t[\"time\"] & dfz_a \n",
    "                    df2 = (df_zscore_t[\"time\"] * df_zscore_a) < 0 \n",
    "                    dfz = df1 & df2 \n",
    "\n",
    "                    index_drop_list = index_drop_list + [*filter(dfz.get, dfz.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_noise_tt(df): \n",
    "    in_len = len(df)\n",
    "    zst = 1.2\n",
    "    \n",
    "    n_prod = df[\"product\"].nunique()\n",
    "    prod_list = df[\"product\"].unique()\n",
    "    n_comp = df[\"company\"].nunique()\n",
    "    comp_list = df[\"company\"].unique()\n",
    "        \n",
    "    print(\"Cleaning noise ... \")\n",
    "    \n",
    "    index_drop_list = []\n",
    "    for prod in prod_list:\n",
    "\n",
    "        df_max_scaled = df[df[\"product\"] == prod].copy()\n",
    "\n",
    "        for comp in comp_list:\n",
    "            df_max_scaled2 = df_max_scaled[df_max_scaled[\"company\"] == comp].copy()\n",
    "\n",
    "            if len(df_max_scaled2) > 1:\n",
    "                \n",
    "                max_min_t = df_max_scaled2[\"time\"].max() - df_max_scaled2[\"time\"].min()\n",
    "                \n",
    "                if (max_min_t != 0):\n",
    "                    df_max_scaled2[\"time\"] = (df_max_scaled2[\"time\"] - df_max_scaled2[\"time\"].min()) / max_min_t\n",
    "                    t_sc = df_max_scaled2[[\"time\"]]\n",
    "                    df_zscore_t = (t_sc - t_sc.mean())/t_sc.std()\n",
    "                    dfz_t = abs(df_zscore_t) > zst\n",
    "                    \n",
    "                    index_drop_list = index_drop_list + dfz_t[dfz_t[\"time\"].eq(True)].index.tolist()\n",
    "                    \n",
    "                    #index_drop_list = index_drop_list + [*filter(dfz_t.get, dfz_t.index)]\n",
    "\n",
    "    index_drop_list = sorted(list(set(index_drop_list)))\n",
    "    df.drop(index_drop_list, axis=0, inplace=True)\n",
    "    rem = len(index_drop_list)\n",
    "    f_len = len(df)\n",
    "    n_del = in_len - f_len\n",
    "    per = (n_del / in_len) * 100\n",
    "    \n",
    "    print(\"deleted indices: \",index_drop_list)\n",
    "    print(\"total number of removed data: \", n_del)\n",
    "    print(\"persentage of removed data: \", round(per, 2), \"%\")\n",
    "    \n",
    "    return df, index_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning noise ... \n",
      "deleted indices:  [95, 134, 248, 252, 256, 279, 328, 339, 407, 433, 434, 457, 468, 518, 554, 555, 573, 578, 590, 606, 611, 616, 658, 689, 898, 913, 1002, 1011, 1016, 1024, 1041, 1051, 1113, 1116, 1118, 1125, 1130, 1151, 1160, 1172, 1233, 1238, 1248, 1254, 1285, 1299, 1301, 1309, 1326, 1380, 1453, 1454, 1540, 1544, 1582, 1664, 1671, 1681, 1707, 1713, 1798, 1810, 1817, 1825, 1841, 1844, 1845, 1855, 1877, 1878, 1879, 1881, 1883, 1894, 1903, 1910, 1921, 1931, 1933, 1963, 1976, 1992, 2007, 2019, 2028, 2034, 2049, 2050, 2052, 2056, 2061, 2064, 2069, 2070, 2095, 2110, 2115, 2119, 2120, 2121, 2123, 2160, 2195, 2202, 2217, 2221, 2228, 2245, 2267, 2269, 2273, 2283, 2327, 2332, 2333, 2338, 2340, 2342, 2346, 2350, 2357, 2359, 2385, 2412, 2413, 2414, 2424, 2434, 2435, 2457, 2460, 2467, 2477, 2478, 2487, 2522, 2540, 2546, 2551, 2567, 2568, 2573, 2584, 2595, 2608, 2613, 2624, 2633, 2634, 2639, 2641, 2644, 2655, 2663, 2682, 2688, 2693, 2698, 2711, 2724, 2727, 2750, 2753, 2767, 2776, 2785, 2788, 2792, 2812, 2821, 2825, 2831, 2843, 2865, 2872, 2873, 2875, 2887, 2895, 2900, 2901, 2907, 2908, 2909, 2912, 2914, 2951, 2952, 2957, 2973, 2993, 3000, 3002, 3009, 3015, 3018, 3025, 3036, 3037, 3040, 3044, 3064, 3070, 3072, 3087, 3098, 3104, 3105, 3109, 3114, 3117, 3131, 3145, 3148, 3155, 3156, 3161, 3181, 3197, 3201, 3215, 3216, 3235, 3269, 3281, 3282, 3297, 3298, 3307, 3340, 3342, 3351, 3363, 3365, 3374, 3415, 3425, 3430, 3433, 3438, 3458, 3464, 3479, 3484, 3487, 3491, 3506, 3507, 3518, 3520, 3523, 3562, 3581, 3590, 3597, 3637, 3652, 3661, 3695, 3766, 3771, 3815, 3848, 3942, 3975, 4026, 4064, 4075, 4085, 4141, 4220, 4222, 4227, 4252, 4287, 4293, 4302, 4303, 4310, 4314, 4317, 4323, 4325, 4366, 4372, 4377, 4378, 4385, 4431, 4439, 4465, 4474, 4502, 4511, 4514, 4544, 4551, 4557, 4570, 4579, 4580, 4584, 4610, 4622, 4646, 4651, 4654, 4659, 4674, 4677, 4693, 4701, 4741, 4742, 4883, 4908, 4922, 4931, 4951, 4952, 5016, 5019, 5099, 5151, 5171, 5202, 5250, 5261, 5270, 5313, 5314, 5327, 5346, 5392, 5402, 5414, 5434, 5441, 5494, 5513, 5515, 5528, 5642, 5755, 5789, 5804, 5869, 5889, 5908, 6073, 6085, 6207, 6220, 6274, 6368, 6436, 6554, 6558, 6658, 6768, 6794, 6796, 6822, 6852, 6918, 6919, 6976, 6978, 7006, 7032, 7050, 7104, 7109, 7116, 7166, 7198, 7199, 7205, 7229, 7258, 7267, 7274]\n",
      "total number of removed data:  382\n",
      "persentage of removed data:  5.66 %\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "data2, train_drop_list = clean_noise(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning noise ... \n",
      "deleted indices:  [2, 3, 23, 30, 58, 69, 92, 97, 99, 102, 105, 106, 108, 125, 126, 135, 158, 167, 192, 205, 206, 215, 229, 247, 281, 282, 297, 303, 326, 329, 331, 340, 347, 355, 358, 396, 401, 402, 403, 408, 421, 422, 425, 427, 430, 444, 447, 448, 458, 469, 477, 484, 494, 497, 498, 517, 520, 525, 530, 533, 539, 545, 549, 551, 561, 568, 569, 574, 579, 585, 593, 596, 597, 601, 602, 609, 615, 622, 631, 641, 642, 643, 646, 674, 678, 679, 683, 685, 688, 701, 715, 716, 728, 736, 741, 747, 755, 760, 767, 772, 781, 810, 822, 828, 835, 837, 838, 845, 883, 903, 941, 960, 980, 981, 994, 997, 1000, 1012, 1021, 1029, 1033, 1063, 1088, 1094, 1105, 1109, 1114, 1115, 1135, 1150, 1167, 1193, 1205, 1249, 1275, 1310, 1318, 1323, 1353, 1398, 1416, 1470, 1490, 1491, 1496, 1513, 1542, 1546, 1560, 1575, 1596, 1625, 1628, 1634, 1635, 1646, 1653, 1662, 1674, 1688, 1734, 1772, 1774, 1785, 1786, 1791, 1793, 1794, 1797, 1803, 1804, 1809, 1811, 1813, 1816, 1823, 1824, 1833, 1834, 1838, 1840, 1846, 1848, 1859, 1862, 1863, 1864, 1869, 1872, 1874, 1875, 1886, 1889, 1891, 1893, 1904, 1905, 1906, 1914, 1926, 1928, 1930, 1937, 1945, 1946, 1947, 1948, 1950, 1951, 1955, 1967, 1969, 1977, 1983, 1985, 1986, 1990, 1991, 1996, 2000, 2005, 2014, 2015, 2035, 2036, 2044, 2047, 2057, 2066, 2067, 2072, 2075, 2076, 2077, 2078, 2080, 2083, 2094, 2098, 2103, 2106, 2107, 2108, 2109, 2117, 2118, 2125, 2127, 2130, 2132, 2135, 2140, 2147, 2149, 2151, 2152, 2153, 2164, 2169, 2172, 2176, 2177, 2178, 2179, 2185, 2190, 2191, 2192, 2194, 2199, 2201, 2203, 2206, 2208, 2211, 2214, 2222, 2224, 2232, 2233, 2234, 2241, 2243, 2246, 2247, 2250, 2272, 2275, 2279, 2281, 2284, 2285, 2291, 2293, 2305, 2309, 2311, 2312, 2313, 2317, 2318, 2322, 2323, 2325, 2326, 2328, 2347, 2348, 2352, 2360, 2372, 2377, 2380, 2381, 2386, 2387, 2392, 2393, 2394, 2396, 2404, 2411, 2415, 2417, 2420, 2423, 2426, 2430, 2433, 2438, 2441, 2442, 2448, 2452, 2459, 2463, 2470, 2472, 2473, 2474, 2481, 2483, 2485, 2491, 2497, 2499, 2509, 2512, 2517, 2518, 2520, 2529, 2537, 2543, 2547, 2549, 2550, 2552, 2557, 2558, 2559, 2566, 2569, 2570, 2571, 2575, 2576, 2578, 2580, 2592, 2596, 2605, 2610, 2612, 2620, 2623, 2635, 2640, 2643, 2645, 2648, 2649, 2650, 2652, 2653, 2654, 2657, 2662, 2668, 2678, 2689, 2691, 2694, 2696, 2697, 2699, 2701, 2706, 2709, 2712, 2714, 2715, 2720, 2738, 2745, 2747, 2749, 2754, 2756, 2758, 2759, 2760, 2762, 2764, 2769, 2770, 2771, 2772, 2782, 2783, 2786, 2791, 2800, 2802, 2805, 2806, 2807, 2813, 2820, 2822, 2827, 2829, 2833, 2834, 2835, 2837, 2841, 2842, 2849, 2852, 2855, 2856, 2860, 2862, 2863, 2877, 2878, 2881, 2886, 2891, 2896, 2898, 2910, 2913, 2915, 2916, 2918, 2919, 2921, 2922, 2928, 2929, 2936, 2944, 2947, 2954, 2962, 2964, 2965, 2966, 2967, 2969, 2970, 2978, 2982, 2989, 2992, 3004, 3011, 3013, 3014, 3017, 3019, 3022, 3027, 3028, 3033, 3039, 3053, 3062, 3065, 3069, 3073, 3076, 3079, 3080, 3081, 3083, 3085, 3086, 3106, 3112, 3118, 3121, 3122, 3127, 3128, 3138, 3140, 3144, 3146, 3149, 3150, 3152, 3157, 3158, 3162, 3168, 3169, 3177, 3178, 3185, 3192, 3195, 3198, 3203, 3204, 3205, 3206, 3208, 3210, 3211, 3213, 3224, 3228, 3236, 3237, 3241, 3244, 3246, 3248, 3250, 3261, 3264, 3266, 3270, 3271, 3272, 3274, 3275, 3276, 3277, 3278, 3280, 3293, 3294, 3299, 3300, 3301, 3303, 3311, 3312, 3317, 3318, 3320, 3324, 3326, 3328, 3329, 3331, 3334, 3339, 3343, 3344, 3356, 3358, 3364, 3368, 3375, 3383, 3385, 3389, 3394, 3395, 3397, 3399, 3402, 3405, 3409, 3416, 3420, 3421, 3422, 3423, 3424, 3429, 3434, 3437, 3440, 3441, 3446, 3456, 3457, 3468, 3472, 3473, 3474, 3476, 3481, 3482, 3490, 3493, 3495, 3501, 3505, 3511, 3512, 3513, 3514, 3521, 3525, 3527, 3535, 3540, 3541, 3545, 3552, 3556, 3564, 3566, 3567, 3593, 3607, 3613, 3653, 3655, 3657, 3658, 3664, 3669, 3674, 3702, 3717, 3728, 3742, 3747, 3755, 3769, 3794, 3800, 3805, 3811, 3817, 3820, 3821, 3831, 3850, 3854, 3863, 3866, 3870, 3882, 3900, 3910, 3939, 3944, 3945, 3949, 3951, 3970, 3984, 3994, 3999, 4012, 4015, 4031, 4054, 4087, 4091, 4093, 4098, 4120, 4139, 4145, 4155, 4162, 4173, 4174, 4175, 4187, 4190, 4194, 4195, 4200, 4204, 4214, 4231, 4248, 4256, 4268, 4274, 4284, 4334, 4336, 4346, 4356, 4363, 4399, 4400, 4405, 4410, 4412, 4458, 4470, 4480, 4487, 4488, 4499, 4526, 4529, 4549, 4555, 4574, 4597, 4617, 4625, 4626, 4627, 4637, 4647, 4667, 4672, 4683, 4689, 4691, 4696, 4712, 4718, 4719, 4721, 4723, 4758, 4760, 4776, 4808, 4812, 4840, 4845, 4868, 4897, 4900, 4910, 4941, 4944, 4955, 4959, 4960, 4980, 4985, 4992, 5002, 5024, 5050, 5059, 5082, 5091, 5100, 5149, 5188, 5192, 5195, 5230, 5241, 5245, 5252, 5262, 5280, 5287, 5300, 5324, 5335, 5339, 5369, 5381, 5382, 5397, 5407, 5413, 5438, 5439, 5447, 5472, 5480, 5496, 5499, 5518, 5530, 5561, 5575, 5576, 5588, 5594, 5604, 5627, 5633, 5643, 5649, 5662, 5667, 5691, 5692, 5707, 5720, 5722, 5724, 5727, 5733, 5734, 5737, 5746, 5761, 5770, 5786, 5792, 5802, 5816, 5822, 5825, 5837, 5847, 5852, 5856, 5863, 5873, 5886, 5911, 5949, 5958, 5972, 5984, 5987, 5990, 6028, 6033, 6047, 6051, 6058, 6083, 6091, 6093, 6103, 6113, 6121, 6143, 6153, 6161, 6162, 6163, 6184, 6191, 6197, 6214, 6223, 6235, 6241, 6249, 6261, 6264, 6288, 6298, 6331, 6351, 6365, 6378, 6396, 6433, 6446, 6454, 6460, 6467, 6471, 6481, 6499, 6516, 6534, 6550, 6581, 6584, 6620, 6621, 6631, 6637, 6649, 6654, 6662, 6704, 6707, 6714, 6717, 6726, 6728, 6739, 6746, 6759, 6763, 6766, 6781, 6782, 6801, 6811, 6840, 6850, 6853, 6872, 6897, 6940, 6951, 6962, 6973, 6982, 6985, 6989, 7010, 7012, 7017, 7033, 7052, 7059, 7063, 7112, 7168, 7179, 7202, 7213, 7223, 7226, 7230, 7233, 7236, 7237, 7250, 7257, 7268, 7277, 7286, 7290, 7314]\n",
      "total number of removed data:  947\n",
      "persentage of removed data:  14.87 %\n"
     ]
    }
   ],
   "source": [
    "data2, train_drop_list = clean_noise_tt(data2)\n",
    "drop_index_list = drop_index_list + train_drop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.604319518953237"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[\"time\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.883828139406233"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[\"time\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379.61"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)*0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132.26829268292684"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2) / len(data2.groupby('time').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "1     2861\n",
       "2      394\n",
       "3      254\n",
       "4      241\n",
       "5      180\n",
       "6      147\n",
       "7      187\n",
       "8      151\n",
       "9      100\n",
       "10      88\n",
       "11      86\n",
       "12      73\n",
       "13      48\n",
       "14      81\n",
       "15      93\n",
       "16      81\n",
       "17      61\n",
       "18      60\n",
       "19      61\n",
       "20      45\n",
       "21      51\n",
       "22       8\n",
       "23      13\n",
       "24       1\n",
       "25       1\n",
       "26      15\n",
       "27       3\n",
       "28       1\n",
       "33       1\n",
       "35       4\n",
       "38       1\n",
       "41       1\n",
       "44       2\n",
       "50       1\n",
       "59       1\n",
       "62       1\n",
       "64       1\n",
       "65       1\n",
       "67      10\n",
       "75       1\n",
       "85      13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.groupby('time').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time(df):\n",
    "    \n",
    "    min_samp = 132 #len(df)*0.07\n",
    "    t = 1\n",
    "    gs = 0\n",
    "    group = []\n",
    "    sub_group = [t]\n",
    "    gap = 5\n",
    "\n",
    "    while t <= df[\"time\"].max():\n",
    "        \n",
    "        \n",
    "        if sum(df[\"time\"] == t) > 0:\n",
    "        \n",
    "            gs += sum(df[\"time\"] == t)\n",
    "\n",
    "            if (len(sub_group) > 0) and ((t - min(sub_group)) <= gap):\n",
    "                sub_group += [t]\n",
    "\n",
    "            else:\n",
    "                if (len(sub_group) != 0):\n",
    "                    group += [sub_group]\n",
    "                sub_group = [t]\n",
    "\n",
    "            if (gs >= min_samp) and ((t - min(sub_group)) <= gap):\n",
    "                gs = 0\n",
    "                group += [sub_group]\n",
    "                sub_group = []\n",
    "                \n",
    "        if t == df[\"time\"].max() :\n",
    "            group += [sub_group]\n",
    "            group[0].remove(1)\n",
    "            \n",
    "            c = True    # son sub_grouptan bir önceki sub_groupa time aktarımı\n",
    "            gap = 5     # son sub_grouptan bir önceki sub_groupa yollanan time değerleri arasındaki maksimum fark \n",
    "            while c:\n",
    "                if len(group[len(group) - 1]) >= 2:\n",
    "                    x = group[len(group)-1][0]\n",
    "                    x_l = group[len(group) - 2] [len(group[len(group)-2]) - 1]\n",
    "                    x_r = group[len(group) - 1] [1]\n",
    "\n",
    "                    if (x - group[len(group)-2][0]) <= gap:\n",
    "                        group[len(group)-2].append(x)\n",
    "                        group[len(group)-1].remove(x)\n",
    "                \n",
    "                x = group[len(group) - 1][0]\n",
    "                if (x - group[len(group)-2][0]) <= gap:\n",
    "                    group[len(group)-2].append(x)\n",
    "                    group[len(group)-1].remove(x)\n",
    "                \n",
    "                c = False\n",
    "        t += 1\n",
    "        \n",
    "    return group\n",
    "\n",
    "\n",
    "############# son sub_groupta kalan datalara bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = map_time(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [2],\n",
       " [3],\n",
       " [4],\n",
       " [5],\n",
       " [6],\n",
       " [7],\n",
       " [8],\n",
       " [9, 10],\n",
       " [11, 12],\n",
       " [13, 14, 15],\n",
       " [16, 17],\n",
       " [18, 19, 20],\n",
       " [21, 22, 23, 24, 25, 26],\n",
       " [27, 28],\n",
       " [33, 35, 38],\n",
       " [41, 44],\n",
       " [50],\n",
       " [59, 62, 64],\n",
       " [65, 67],\n",
       " [75],\n",
       " [85]]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msr\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sr' is not defined"
     ]
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time(df):\n",
    "    \n",
    "    min_samp = 5\n",
    "    t = 0\n",
    "    gs = 0\n",
    "    group = []\n",
    "    sub_group = []\n",
    "    \n",
    "    while t <= df[\"time\"].max():\n",
    "        \n",
    "        t += 1\n",
    "        if sum(df[\"time\"]==t) > 0\n",
    "        \n",
    "            sub_group = [t]\n",
    "            gs += sum(df[\"time\"]==t)\n",
    "\n",
    "            if (len(sub_group) > 0) and ((max(sub_group) - min(sub_group)) < 7):\n",
    "                sub_group += [t]\n",
    "                print(\"a\")\n",
    "            else:\n",
    "                if len(sub_group) != 0:\n",
    "                    group += [sub_group]\n",
    "                sub_group = [t]\n",
    "                print(\"b\")\n",
    "\n",
    "            if (gs >= min_samp):\n",
    "                print(\"c\")\n",
    "                gs = 0\n",
    "                group += [sub_group]\n",
    "                sub_group = []\n",
    "            \n",
    "        \n",
    "        \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time(df):\n",
    "    for i in df.index:\n",
    "        if ((df.at[i, \"time\"]<=7) and (df.at[i, \"time\"]>=4)):\n",
    "            df.at[i, \"time\"] = 4\n",
    "        elif (df.at[i, \"time\"]>7):\n",
    "            df.at[i, \"time\"] = 5\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_time(df):\n",
    "    for i in df.index:\n",
    "        if ((df.at[i, \"time\"]<=4) and (df.at[i, \"time\"]>=3)):\n",
    "            df.at[i, \"time\"] = 3\n",
    "        elif ((df.at[i, \"time\"]<=7) and (df.at[i, \"time\"]>=5)):\n",
    "            df.at[i, \"time\"] = 4\n",
    "        elif ((df.at[i, \"time\"]<=14) and (df.at[i, \"time\"]>=8)):\n",
    "            df.at[i, \"time\"] = 5\n",
    "        elif ((df.at[i, \"time\"]<=30) and (df.at[i, \"time\"]>=15)):\n",
    "            df.at[i, \"time\"] = 6\n",
    "        elif (df.at[i, \"time\"]>30):\n",
    "            df.at[i, \"time\"] = 7\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = map_time(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One - Hot - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(df, ft):      ### ft = \"company\", \"product\", \"week day\" etc.\n",
    "    print(\"one hot encoding \", ft, \"...\")\n",
    "    dum = pd.get_dummies(df[ft])\n",
    "    df = df.drop(ft, axis = 1)\n",
    "    df = df.join(dum)\n",
    "    print(ft, \"encoded.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data2  = one_hot(data2, \"week day\")\n",
    "data2  = one_hot(data2, \"company\")\n",
    "data2  = one_hot(data2, \"product\")\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2.drop(train_drop_list, axis=0, inplace=True)\n",
    "#data2.drop(val_drop_list, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index_list = sorted(list(set(drop_index_list)))\n",
    "drop_df = drop_df.loc[drop_index_list]\n",
    "drop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.drop(drop_df.index.to_list(), axis=0, inplace=True)\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop town column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.drop('town',axis = 1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import EditedNearestNeighbours \n",
    "from collections import Counter\n",
    "\n",
    "xt = data2.copy()\n",
    "yt = data2.copy()\n",
    "xt.drop(\"time\", axis=1, inplace=True)\n",
    "yt = yt[\"time\"] \n",
    "\n",
    "\n",
    "print('Original dataset shape %s' % Counter(xt))\n",
    "print(f'Original dataset samples per class {Counter(yt)}')\n",
    "\n",
    "sm_obj = SMOTE(random_state=42, sampling_strategy='all', k_neighbors=4, n_jobs=-1)\n",
    "enn_obj = EditedNearestNeighbours(sampling_strategy='all', n_neighbors=3, kind_sel='mode', n_jobs=-1)\n",
    "sme = SMOTEENN(random_state=42, smote=sm_obj, enn=enn_obj, sampling_strategy='all', n_jobs=-1)\n",
    "\n",
    "X_res, y_res = sme.fit_resample(xt, yt)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))\n",
    "print(f'Resampled dataset shape {X_res.shape}')\n",
    "x_train, y_train = X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data2.copy()\n",
    "Y = data2.copy()\n",
    "X.drop(\"time\", axis=1, inplace=True)\n",
    "Y = Y[\"time\"]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X,Y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(df):\n",
    "    ls = df.columns.to_list()\n",
    "    ls.remove(\"amount\")\n",
    "    df[ls] = df[ls].astype('category')\n",
    "#     df['time'] = df['time'].cat.rename_categories({1:\"1 gün\", 2:\"2 gün\", 3:\"3-4 gün\", 4:\"5-7 gün\", 5:\"8-14 gün\", 6:\"15-30 gün\", 7:\"+30 gün\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = categorize(x_train)\n",
    "# x_val = categorize(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_min = x_train[\"amount\"].min()\n",
    "xt_max = x_train[\"amount\"].max()\n",
    "\n",
    "x_train[\"amount\"] = (x_train[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "x_val[\"amount\"] = (x_val[\"amount\"] - xt_min) / (xt_max - xt_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a df_empty with the same columns of x_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empty = x_train[0:0]\n",
    "df_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(x_train, x_val):\n",
    "    from sklearn.decomposition import PCA\n",
    "    xl = len(x_train.columns)\n",
    "    pca = PCA(.95)\n",
    "    pca.fit(x_train)\n",
    "    print(\"number of features dropped from \", xl, \" to \", pca.n_components_) \n",
    "    #print(\"variance ratio: \", pca.explained_variance_ratio_) \n",
    "\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_val = pca.transform(x_val)\n",
    "    return pca, x_train, x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTENC\n",
    "##### before pca for sure (there are categorical fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_fts(df):\n",
    "    fts = list(df.columns)\n",
    "    fts.remove('amount')\n",
    "    return fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTENC\n",
    "# from collections import Counter\n",
    "\n",
    "# print(f'Original dataset shape {x_train.shape}')\n",
    "# print(f'Original dataset samples per class {Counter(y_train)}')\n",
    "\n",
    "# # categorical features\n",
    "# fts = list(range(1, ((x_train.shape[1]))))\n",
    "\n",
    "# sm = SMOTENC(random_state=42, categorical_features=fts, sampling_strategy='all', k_neighbors=4, n_jobs=-1)\n",
    "# X_res, y_res = sm.fit_resample(x_train, y_train)\n",
    "# print(f'Resampled dataset samples per class {Counter(y_res)}')\n",
    "# print(f'Resampled dataset shape {X_res.shape}')\n",
    "# x_train, y_train = X_res, y_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTEENN\n",
    "##### after pca?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, x_train, x_val = do_pca(x_train, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTEENN\n",
    "# from imblearn.over_sampling import SMOTE \n",
    "# from imblearn.under_sampling import EditedNearestNeighbours \n",
    "# from collections import Counter\n",
    "# print('Original dataset shape %s' % Counter(y_train))\n",
    "# print(f'Original dataset samples per class {Counter(y_train)}')\n",
    "\n",
    "# sm_obj = SMOTE(random_state=42, sampling_strategy='all', k_neighbors=4, n_jobs=-1)\n",
    "# enn_obj = EditedNearestNeighbours(sampling_strategy='all', n_neighbors=3, kind_sel='mode', n_jobs=-1)\n",
    "# sme = SMOTEENN(random_state=42, smote=sm_obj, enn=enn_obj, sampling_strategy='all', n_jobs=-1)\n",
    "# X_res, y_res = sme.fit_resample(x_train, y_train)\n",
    "# print('Resampled dataset shape %s' % Counter(y_res))\n",
    "# print(f'Resampled dataset shape {X_res.shape}')\n",
    "# x_train, y_train = X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_depth': 7.778449929951673,\n",
    " 'min_samples_leaf': 3.343093555340267,\n",
    " 'min_samples_split': 4.450238161602154,\n",
    " 'n_estimators': 267.8280516713154}\n",
    "\n",
    "params[\"min_samples_leaf\"] = round(params[\"min_samples_leaf\"])\n",
    "params[\"min_samples_split\"] = round(params[\"min_samples_split\"])\n",
    "params[\"n_estimators\"] = round(params[\"n_estimators\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndmForest = RandomForestClassifier(random_state=42).fit(x_train, y_train)\n",
    "rndmForest_opt = RandomForestClassifier(random_state=42, **params).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndmForest_predictions = rndmForest.predict(x_val)\n",
    "rndmForest_opt_predictions = rndmForest_opt.predict(x_val)\n",
    "\n",
    "print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_val, rndmForest_predictions))\n",
    "print('Accuracy of RandomForest_opt classifier: ', accuracy_calculate(y_val, rndmForest_opt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, rndmForest_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, rndmForest_opt_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "\n",
    "Logistic_Regression = LogisticRegression(random_state= random_state, multi_class='multinomial', max_iter = 1000).fit(x_train, y_train.astype('int'))\n",
    "#linearReg = LinearRegression().fit(x_train, y_train.astype('int'))\n",
    "rndmForest = RandomForestClassifier(n_estimators=1000, class_weight=\"balanced\", n_jobs=-1, min_samples_leaf=3, max_depth=5,\n",
    "                                    random_state=random_state).fit(x_train, y_train.astype('int'))\n",
    "                                                                   \n",
    "#MLP = MLPClassifier(random_state=1, max_iter=5000).fit(x_train, y_train.astype('int'))\n",
    "\n",
    "#Finding k value fom max accuracy\n",
    "k_values=[]\n",
    "for k in range(1, 51):\n",
    "    KNeighbors = KNeighborsClassifier(n_neighbors=k, weights='distance').fit(x_train, y_train.astype('int'))\n",
    "    KNeighbors_predictions = KNeighbors.predict(x_val)\n",
    "    k_values.append(balanced_accuracy_score(y_val, KNeighbors_predictions))\n",
    "k_max = k_values.index(max(k_values)) + 1\n",
    "\n",
    "KNeighbors = KNeighborsClassifier(n_neighbors=k_max).fit(x_train, y_train.astype('int'))\n",
    "CSupportVector = make_pipeline(StandardScaler(), SVC(gamma='auto')).fit(x_train, y_train.astype('int'))\n",
    "DecisionTtree = DecisionTreeClassifier(random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# GaussianProcess = GaussianProcessClassifier(kernel=kernel,random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# AdaBoost = AdaBoostClassifier(n_estimators=1000, random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# GaussianNaiveBayes = GaussianNB().fit(x_train, y_train.astype('int'))\n",
    "# QuadraticDiscriminantAnalysis = QuadraticDiscriminantAnalysis().fit(x_train, y_train.astype('int'))\n",
    "\n",
    "\n",
    "LogisticRegression_predictions = Logistic_Regression.predict(x_train)\n",
    "#linearReg_predictions = linearReg.predict(x_val)\n",
    "rndmForest_predictions = rndmForest.predict(x_train)\n",
    "#MLP_predictions = MLP.predict(x_val)\n",
    "KNeighbors_predictions = KNeighbors.predict(x_train)\n",
    "CSupportVector_predictions = CSupportVector.predict(x_train)\n",
    "DecisionTtree_predictions = DecisionTtree.predict(x_train)\n",
    "# GaussianProcess_predictions = GaussianProcess.predict(x_val)\n",
    "# AdaBoost_predictions = AdaBoost.predict(x_val)\n",
    "# GaussianNaiveBayes_predictions = GaussianNaiveBayes.predict(x_val)\n",
    "# QuadraticDiscriminantAnalysis_predictions = QuadraticDiscriminantAnalysis.predict(x_val)\n",
    "\n",
    "\n",
    "print('Accuracy of LogisticRegression classifier: ', accuracy_calculate(y_train, LogisticRegression_predictions))\n",
    "#print('Accuracy of LinearRegression classifier: ', accuracy_calculate(y_val, linearReg_predictions))\n",
    "print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_train, rndmForest_predictions))\n",
    "#print('Accuracy of Multi-layer Perceptron classifier: ', accuracy_calculate(y_val, MLP_predictions))\n",
    "print('Accuracy of KNeighbors classifier: ', accuracy_calculate(y_train, KNeighbors_predictions))\n",
    "print('Accuracy of CSupportVector classifier: ', accuracy_calculate(y_train, CSupportVector_predictions))\n",
    "print('Accuracy of DecisionTtree classifier: ', accuracy_calculate(y_train, DecisionTtree_predictions))\n",
    "#print('Accuracy of GaussianProcess classifier: ', accuracy_calculate(y_val, GaussianProcess_predictions))\n",
    "#print('Accuracy of AdaBoost classifier: ', accuracy_calculate(y_val, AdaBoost_predictions))\n",
    "#print('Accuracy of GaussianNaiveBayes classifier: ', accuracy_calculate(y_val, GaussianNaiveBayes_predictions))\n",
    "#print('Accuracy of QuadraticDiscriminantAnalysis classifier: ', accuracy_calculate(y_val, QuadraticDiscriminantAnalysis_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Accuracy / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Balanced_Accuracy of LogisticRegression classifier: ', balanced_accuracy_score(y_train, LogisticRegression_predictions))\n",
    "print('Balanced_Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_train, rndmForest_predictions))\n",
    "print('Balanced_Accuracy of KNeighbors classifier: ', balanced_accuracy_score(y_train, KNeighbors_predictions))\n",
    "print('Balanced_Accuracy of CSupportVector classifier: ', balanced_accuracy_score(y_train, CSupportVector_predictions))\n",
    "print('Balanced_Accuracy of DecisionTtree classifier: ', balanced_accuracy_score(y_train, DecisionTtree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DENİYOZ İŞTE #################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTtree = DecisionTreeClassifier(random_state=0, criterion='gini', min_samples_leaf=3, max_depth=20).fit(x_train, y_train.astype('int'))\n",
    "# DecisionTtree_predictions = DecisionTtree.predict(x_train)\n",
    "# print('Accuracy of DecisionTtree_predictions : ', accuracy_calculate(y_train, DecisionTtree_predictions))\n",
    "# print('Balanced_Accuracy of DecisionTtree_predictions : ', balanced_accuracy_score(y_train, DecisionTtree_predictions))\n",
    "\n",
    "# DecisionTtree_predictions = DecisionTtree.predict(x_val)\n",
    "# print('Accuracy of DecisionTtree_predictions : ', accuracy_calculate(y_val, DecisionTtree_predictions))\n",
    "# print('Balanced_Accuracy of DecisionTtree_predictions : ', balanced_accuracy_score(y_val, DecisionTtree_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic_Regression = LogisticRegression(class_weight=\"balanced\", multi_class=\"ovr\", random_state= random_state, max_iter = 1000).fit(x_train, y_train.astype('int'))\n",
    "# LogisticRegression_predictions = Logistic_Regression.predict(x_train)\n",
    "# print('Accuracy of LogisticRegression classifier: ', accuracy_calculate(y_train, LogisticRegression_predictions))\n",
    "# print('Balanced_Accuracy of LogisticRegression classifier: ', balanced_accuracy_score(y_train, LogisticRegression_predictions))\n",
    "\n",
    "# LogisticRegression_predictions = Logistic_Regression.predict(x_val)\n",
    "# print('Accuracy of LogisticRegression classifier: ', accuracy_calculate(y_val, LogisticRegression_predictions))\n",
    "# print('Balanced_Accuracy of LogisticRegression classifier: ', balanced_accuracy_score(y_val, LogisticRegression_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = SGDClassifier(loss='log',class_weight=\"balanced\", max_iter=2000).fit(x_train, y_train)\n",
    "# predictions = svm.predict(x_val)\n",
    "# print(\"accuracy: \",accuracy_score(predictions, y_val))\n",
    "# print(\"accuracy: \",balanced_accuracy_score(predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_train, LogisticRegression_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_train, LogisticRegression_predictions))\n",
    "print(\"precision: \",precision_score(y_train, LogisticRegression_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_train, LogisticRegression_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_train, LogisticRegression_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_train, LogisticRegression_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_train, LogisticRegression_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_train, rndmForest_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_train, rndmForest_predictions))\n",
    "print(\"precision: \",precision_score(y_train, rndmForest_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_train, rndmForest_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_train, rndmForest_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_train, rndmForest_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_train, rndmForest_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier  / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_train, KNeighbors_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_train, KNeighbors_predictions))\n",
    "print(\"precision: \",precision_score(y_train, KNeighbors_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_train, KNeighbors_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_train, KNeighbors_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_train, KNeighbors_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_train, rndmForest_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSupportVector  / train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_train, CSupportVector_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_train, CSupportVector_predictions))\n",
    "print(\"precision: \",precision_score(y_train, CSupportVector_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_train, CSupportVector_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_train, CSupportVector_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_train, CSupportVector_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_train, CSupportVector_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random_state = 42\n",
    "# # kernel = 1.0 * RBF(1.0)\n",
    "\n",
    "# # Logistic_Regression = LogisticRegression(random_state= random_state, max_iter = 1000).fit(x_train, y_train.astype('int'))\n",
    "# # #linearReg = LinearRegression().fit(x_train, y_train.astype('int'))\n",
    "# # rndmForest = RandomForestClassifier(n_estimators=1000, class_weight=\"balanced\", n_jobs=-1, min_samples_leaf=3, max_depth=5,\n",
    "# #                                     random_state=random_state).fit(x_train, y_train.astype('int'))\n",
    "                                                                   \n",
    "# # #MLP = MLPClassifier(random_state=1, max_iter=5000).fit(x_train, y_train.astype('int'))\n",
    "\n",
    "# # #Finding k value fom max accuracy\n",
    "# # k_values=[]\n",
    "# # for k in range(1, 51):\n",
    "# #     KNeighbors = KNeighborsClassifier(n_neighbors=k).fit(x_train, y_train.astype('int'))\n",
    "# #     KNeighbors_predictions = KNeighbors.predict(x_val)\n",
    "# #     k_values.append(accuracy_calculate(y_val, KNeighbors_predictions))\n",
    "\n",
    "# # k_max = k_values.index(max(k_values)) + 1\n",
    "# KNeighbors = KNeighborsClassifier(n_neighbors=k_max).fit(x_train, y_train.astype('int'))\n",
    "# CSupportVector = make_pipeline(StandardScaler(), SVC(gamma='auto')).fit(x_train, y_train.astype('int'))\n",
    "# DecisionTtree = DecisionTreeClassifier(random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# # GaussianProcess = GaussianProcessClassifier(kernel=kernel,random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# # AdaBoost = AdaBoostClassifier(n_estimators=1000, random_state=0).fit(x_train, y_train.astype('int'))\n",
    "# # GaussianNaiveBayes = GaussianNB().fit(x_train, y_train.astype('int'))\n",
    "# # QuadraticDiscriminantAnalysis = QuadraticDiscriminantAnalysis().fit(x_train, y_train.astype('int'))\n",
    "\n",
    "\n",
    "LogisticRegression_predictions = Logistic_Regression.predict(x_val)\n",
    "#linearReg_predictions = linearReg.predict(x_val)\n",
    "rndmForest_predictions = rndmForest.predict(x_val)\n",
    "#MLP_predictions = MLP.predict(x_val)\n",
    "KNeighbors_predictions = KNeighbors.predict(x_val)\n",
    "CSupportVector_predictions = CSupportVector.predict(x_val)\n",
    "DecisionTtree_predictions = DecisionTtree.predict(x_val)\n",
    "# GaussianProcess_predictions = GaussianProcess.predict(x_val)\n",
    "# AdaBoost_predictions = AdaBoost.predict(x_val)\n",
    "# GaussianNaiveBayes_predictions = GaussianNaiveBayes.predict(x_val)\n",
    "# QuadraticDiscriminantAnalysis_predictions = QuadraticDiscriminantAnalysis.predict(x_val)\n",
    "\n",
    "\n",
    "print('Accuracy of LogisticRegression classifier: ', accuracy_calculate(y_val, LogisticRegression_predictions))\n",
    "#print('Accuracy of LinearRegression classifier: ', accuracy_calculate(y_val, linearReg_predictions))\n",
    "print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_val, rndmForest_predictions))\n",
    "#print('Accuracy of Multi-layer Perceptron classifier: ', accuracy_calculate(y_val, MLP_predictions))\n",
    "print('Accuracy of KNeighbors classifier: ', accuracy_calculate(y_val, KNeighbors_predictions))\n",
    "print('Accuracy of CSupportVector classifier: ', accuracy_calculate(y_val, CSupportVector_predictions))\n",
    "print('Accuracy of DecisionTtree classifier: ', accuracy_calculate(y_val, DecisionTtree_predictions))\n",
    "#print('Accuracy of GaussianProcess classifier: ', accuracy_calculate(y_val, GaussianProcess_predictions))\n",
    "#print('Accuracy of AdaBoost classifier: ', accuracy_calculate(y_val, AdaBoost_predictions))\n",
    "#print('Accuracy of GaussianNaiveBayes classifier: ', accuracy_calculate(y_val, GaussianNaiveBayes_predictions))\n",
    "#print('Accuracy of QuadraticDiscriminantAnalysis classifier: ', accuracy_calculate(y_val, QuadraticDiscriminantAnalysis_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rndmForest = RandomForestClassifier(n_estimators=1000, class_weight=\"balanced\", n_jobs=-1, min_samples_leaf=3, max_depth=5,\n",
    "#                                     random_state=random_state).fit(x_train, y_train.astype('int'))\n",
    "# rndmForest_predictions = rndmForest.predict(x_val)\n",
    "# print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_val, rndmForest_predictions))\n",
    "# print('Balanced_Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_val, rndmForest_predictions))\n",
    "# rndmForest_predictions = rndmForest.predict(x_train)\n",
    "# print('Accuracy of RandomForest classifier: ', accuracy_calculate(y_train, rndmForest_predictions))\n",
    "# print('Balanced_Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_train, rndmForest_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Accuracy / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Balanced_Accuracy of LogisticRegression classifier: ', balanced_accuracy_score(y_val, LogisticRegression_predictions))\n",
    "print('Balanced_Accuracy of RandomForest classifier: ', balanced_accuracy_score(y_val, rndmForest_predictions))\n",
    "print('Balanced_Accuracy of KNeighbors classifier: ', balanced_accuracy_score(y_val, KNeighbors_predictions))\n",
    "print('Balanced_Accuracy of CSupportVector classifier: ', balanced_accuracy_score(y_val, CSupportVector_predictions))\n",
    "print('Balanced_Accuracy of DecisionTtree classifier: ', balanced_accuracy_score(y_val, DecisionTtree_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_val, LogisticRegression_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_val, LogisticRegression_predictions))\n",
    "print(\"precision: \",precision_score(y_val, LogisticRegression_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_val, LogisticRegression_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_val, LogisticRegression_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_val, LogisticRegression_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_val, LogisticRegression_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_val, rndmForest_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_val, rndmForest_predictions))\n",
    "print(\"precision: \",precision_score(y_val, rndmForest_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_val, rndmForest_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_val, rndmForest_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_val, rndmForest_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_val, rndmForest_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_val, KNeighbors_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_val, KNeighbors_predictions))\n",
    "print(\"precision: \",precision_score(y_val, KNeighbors_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_val, KNeighbors_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_val, KNeighbors_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_val, KNeighbors_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_val, KNeighbors_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSupportVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"balanced_accuracy: \",balanced_accuracy_score(y_val, CSupportVector_predictions))\n",
    "print(\"accuracy: \",accuracy_score(y_val, CSupportVector_predictions))\n",
    "print(\"precision: \",precision_score(y_val, CSupportVector_predictions,average='weighted'))\n",
    "print(\"recall: \",recall_score(y_val, CSupportVector_predictions,average='weighted'))\n",
    "print(\"f1 score: \",f1_score(y_val, CSupportVector_predictions,average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_val, CSupportVector_predictions)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_iou_calculator(y_val, CSupportVector_predictions, \"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take input and create df_inp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prod_in_data(df):\n",
    "    prods = df[\"product\"].unique()\n",
    "    prd = str(input(\"Product seç: \"))\n",
    "    if prd not in prods: \n",
    "        return False, prd\n",
    "    else:\n",
    "        return True, prd\n",
    "    \n",
    "def is_prod_in_data_drop(df, prd):\n",
    "    prods = df[\"product\"].unique()\n",
    "    if prd not in prods: \n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_input(df, prd):\n",
    "    amo = input(\"Amount: \") \n",
    "    y = False\n",
    "    while y == False:\n",
    "        if (amo.isnumeric() == False):\n",
    "            print(\"Pozitif tam sayı değer giriniz\")\n",
    "            amo = input(\"Amount: \")\n",
    "\n",
    "        else:\n",
    "            d = max(df[df[\"product\"] == prd][\"amount\"].to_list()) * 3\n",
    "            if int(amo) > d:\n",
    "                print(\"Amount yüksek abi emin misin bak !?\")\n",
    "                y_n = input(\"y / n ?\")\n",
    "                if y_n == \"y\":\n",
    "                    y = True\n",
    "                elif y_n == \"n\":\n",
    "                    amo = input(\"Amount: \")                   \n",
    "            else:\n",
    "                y = True\n",
    "    amo = int(amo)\n",
    "\n",
    "    w_days = ['pts', 'sal', 'çrş', 'prş', 'cum', 'cts', 'paz']\n",
    "    wd = str(input(\"Week day: \"))\n",
    "    z = False\n",
    "    while z == False:\n",
    "        if wd not in w_days:\n",
    "            print(\"Geçerli gün giriniz...\")\n",
    "            print(\"Geçerli günler: \", w_days)\n",
    "            wd = str(input(\"Week day: \"))\n",
    "        else:\n",
    "            z = True\n",
    "\n",
    "    typ = df[df[\"product\"] == prd][\"type\"].unique()[0] \n",
    "    comps = df[df[\"product\"] == prd][\"company\"].unique()\n",
    "    tws = df[df[\"product\"] == prd][\"town\"].unique()\n",
    "\n",
    "    df_inp = pd.DataFrame(columns = ['product', 'company', 'amount', 'town', 'type', 'week day'])\n",
    "\n",
    "    for comp in comps:\n",
    "        tw = df[(df[\"product\"] == prd) & (df[\"company\"] == comp)][\"town\"].unique()[0]\n",
    "\n",
    "        df_inp = df_inp.append({'product' : prd, 'company' : comp, 'amount' : amo, 'town' : tw,'type' : typ,\n",
    "                            'week day' : wd}, ignore_index=True)\n",
    "\n",
    "    return df_inp, comps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Input !!!!!!!!!!!!\n",
    "##### yetersiz datalardaysa ve farklı şirketlerden tedarik edilmişse ayrı ayrı ortalama süre verilebilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    #c = True\n",
    "    cond1 = False ###\n",
    "    #while c:\n",
    "    while cond1 == False: ###\n",
    "        tf, prd = is_prod_in_data(data_clean)\n",
    "        \n",
    "        if tf:\n",
    "            df_inp, comps = take_input(data_clean, prd)\n",
    "            c = False\n",
    "            cond1 = True\n",
    "        else:\n",
    "            if (is_prod_in_data_drop(drop_df, prd) == True):\n",
    "                print(\"Güvenilir sonuç için product'a ait en az 5 giriş bulunmalıdır.\")\n",
    "                print(\"\\n\",\"Daha önce bu product alımları: \")\n",
    "                a = drop_df[drop_df[\"product\"] == prd]\n",
    "                a = a.index.to_list()\n",
    "                a = data.loc[a]\n",
    "                df_inp = display(a[['company', 'amount', 'town', 'order date', 'delivery date', 'time']].style.hide(axis='index'))\n",
    "                print (\"Ortalama miktar = \", a[\"amount\"].mean(), \"Ortalama süre = \", a[\"time\"].mean())\n",
    "                comps = a[\"company\"].unique()\n",
    "                return df_inp, cond1, comps\n",
    "    \n",
    "            else:\n",
    "                df_inp = print(\"Product bulunamadı.\")\n",
    "    if cond1:\n",
    "        df_inp  = one_hot(df_inp, \"week day\")\n",
    "        df_inp  = one_hot(df_inp, \"product\")\n",
    "        df_inp  = one_hot(df_inp, \"company\")\n",
    "\n",
    "        df_inp = df_inp.drop('town',axis = 1).reset_index(drop=True)\n",
    "\n",
    "        df_inp[\"amount\"] = (df_inp[\"amount\"] - xt_min) / (xt_max - xt_min)\n",
    "\n",
    "        df_inp = df_empty.append(df_inp)   # for the next version of pandas use the next line of code instead of this one. \n",
    "        #df_inp = pd.concat([df_inp, df_empty])   ------->>> for the future version of pandas\n",
    "        \n",
    "        df_inp = df_inp.fillna(0)\n",
    "        df_inp = pca.transform(df_inp)\n",
    "        \n",
    "    return df_inp, cond1, comps\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inp, cond1, comps = run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cond1:\n",
    "    LR = Logistic_Regression.predict(df_inp)\n",
    "    RF = rndmForest.predict(df_inp)\n",
    "    KN = KNeighbors.predict(df_inp)\n",
    "    CSV = CSupportVector.predict(df_inp)\n",
    "    print(\"LR: \", LR, \"\\nRF:\", RF, \"\\nKN:\", KN, \"\\nCSV:\", CSV)\n",
    "    \n",
    "    res = np.array([])\n",
    "    output = np.array([])\n",
    "    for i in range(len(LR)):\n",
    "        res = np.append(res, [LR[i], RF[i], KN[i], CSV[i]])\n",
    "        m = mode(res)[0][0]\n",
    "        output = np.append(output, m)\n",
    "        res = np.array([])\n",
    "    \n",
    "    print(\"for company \", comps[i], \" predicted time = \", output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df_inp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
